{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/3/31 18:23\n",
    "# @Author  : 银尘\n",
    "# @FileName: virtual-dast.py\n",
    "# @Software: PyCharm\n",
    "# @Email   : liwudi@liwudi.fun\n",
    "# @Info    : why create this file\n",
    "import argparse\n",
    "import ast\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PaperCrawlerUtil.common_util import *\n",
    "from PaperCrawlerUtil.constant import *\n",
    "from PaperCrawlerUtil.crawler_util import *\n",
    "from dgl.nn import GATConv\n",
    "from dtaidistance import dtw\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model import *\n",
    "from funcs import *\n",
    "from params_ipynb import *\n",
    "from util import *\n",
    "from PaperCrawlerUtil.research_util import *\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.data import MyDataLoader\n",
    "from utils.funcs import load_data, load_all_adj, StandardScaler, get_target_loader\n",
    "from utils.funcs import masked_loss\n",
    "from utils.vec import generate_vector\n",
    "from model import DASTNet, Domain_classifier_DG\n",
    "from PaperCrawlerUtil.common_util import *\n",
    "from PaperCrawlerUtil.research_util import *\n",
    "import ast\n",
    "\n",
    "basic_config(logs_style=LOG_STYLE_ALL)\n",
    "p_bar = process_bar(final_prompt=\"初始化准备完成\", unit=\"part\")\n",
    "long_term_save = {}\n",
    "args = params()\n",
    "long_term_save[\"args\"] = args.__str__()\n",
    "if args.c != \"default\":\n",
    "    c = ast.literal_eval(args.c)\n",
    "    record = ResearchRecord(**c)\n",
    "    record_id = record.insert(__file__, get_timestamp(), args.__str__())\n",
    "p_bar.process(0, 1, 5)\n",
    "source_emb_label2, source_t_adj, source_edge_labels2, lag, source_poi, source_data2, \\\n",
    "source_train_y, source_test_x, source_val_x, source_poi_adj, source_poi_adj2, dataname, target_train_x, \\\n",
    "th_mask_source2, th_mask_source, target_test_loader, target_poi, target_od_adj, \\\n",
    "source_dataset, mask_source, target_graphs, target_val_dataset, max_val, scity2, smin2, \\\n",
    "target_emb_label, tcity, source_road_adj2, gpu_available, source_edges2, \\\n",
    "mask_source2, source_poi_cos, source_data, source_graphs, lng_source, source_road_adj, target_d_adj, \\\n",
    "target_val_x, source_poi2, scity, target_t_adj, lat_source, lat_target, target_test_x, \\\n",
    "source_x, target_val_y, lng_source2, num_tuine_epochs, source_d_adj, source_edge_labels, source_prox_adj, \\\n",
    "source_loader, source_graphs2, transform, source_t_adj2, smax2, target_train_loader, \\\n",
    "source_test_dataset2, source_poi_cos2, source_od_adj2, target_s_adj, target_test_dataset, \\\n",
    "source_test_y2, source_y, source_dataset2, target_road_adj, source_test_loader, target_poi_adj, \\\n",
    "smax, start_time, target_test_y, lng_target, source_test_loader2, \\\n",
    "source_prox_adj2, target_data, source_x2, target_train_dataset, source_test_dataset, source_test_x2, source_od_adj, target_val_loader, smin, target_poi_cos, target_edge_labels, \\\n",
    "source_edges, source_train_x2, source_s_adj, source_y2, source_val_x2, source_emb_label, \\\n",
    "target_norm_poi, source_norm_poi, source_train_x, datatype, source_val_y, mask_target, \\\n",
    "source_train_y2, source_norm_poi2, source_s_adj2, num_epochs, lat_source2, min_val, target_edges, \\\n",
    "source_val_y2, target_prox_adj, source_loader2, source_test_y, source_d_adj, \\\n",
    "target_train_y, th_mask_target, device, p_bar = load_process_data(args, p_bar)\n",
    "\n",
    "if args.need_third == 1:\n",
    "    scity3 = args.scity3\n",
    "    source_data3 = np.load(\". /data/%s/%s%s_%s.npy\" % (scity3, dataname, scity3, datatype))\n",
    "    lng_source3, lat_source3 = source_data3.shape[1], source_data3.shape[2]\n",
    "    mask_source3 = source_data3.sum(0) > 0\n",
    "    th_mask_source3 = torch.Tensor(mask_source3.reshape(1, lng_source3, lat_source3)).to(device)\n",
    "    log(\"%d valid regions in source3\" % np.sum(mask_source3))\n",
    "    # 按照百分比分配标签\n",
    "    source_emb_label3 = masked_percentile_label(source_data3.sum(0).reshape(-1), mask_source3.reshape(-1))\n",
    "    lag = [-6, -5, -4, -3, -2, -1]\n",
    "    source_data3, smax3, smin3 = min_max_normalize(source_data3)\n",
    "    source_train_x3, source_train_y3, source_val_x3, source_val_y3, source_test_x3, source_test_y3 = split_x_y(\n",
    "        source_data3,\n",
    "        lag)\n",
    "    # we concatenate all source data\n",
    "    source_x3 = np.concatenate([source_train_x3, source_val_x3, source_test_x3], axis=0)\n",
    "    source_y3 = np.concatenate([source_train_y3, source_val_y3, source_test_y3], axis=0)\n",
    "    source_test_dataset3 = TensorDataset(torch.Tensor(source_test_x3), torch.Tensor(source_test_y3))\n",
    "    source_test_loader3 = DataLoader(source_test_dataset3, batch_size=args.batch_size)\n",
    "    source_dataset3 = TensorDataset(torch.Tensor(source_x3), torch.Tensor(source_y3))\n",
    "    source_loader3 = DataLoader(source_dataset3, batch_size=args.batch_size, shuffle=True)\n",
    "    source_poi3 = np.load(\"./data/%s/%s_poi.npy\" % (scity3, scity3))\n",
    "    source_poi3 = source_poi3.reshape(lng_source3 * lat_source3, -1)\n",
    "    transform3 = TfidfTransformer()\n",
    "    source_norm_poi3 = np.array(transform3.fit_transform(source_poi3).todense())\n",
    "    source_prox_adj3 = add_self_loop(build_prox_graph(lng_source3, lat_source3))\n",
    "    source_road_adj3 = add_self_loop(build_road_graph(scity3, lng_source3, lat_source3))\n",
    "    source_poi_adj3, source_poi_cos3 = build_poi_graph(source_norm_poi3, args.topk)\n",
    "    source_poi_adj3 = add_self_loop(source_poi_adj3)\n",
    "    source_s_adj3, source_d_adj3, source_od_adj3 = build_source_dest_graph(scity3, dataname, lng_source3, lat_source3,\n",
    "                                                                           args.topk)\n",
    "    source_s_adj3 = add_self_loop(source_s_adj3)\n",
    "    source_t_adj3 = add_self_loop(source_d_adj3)\n",
    "    source_od_adj3 = add_self_loop(source_od_adj3)\n",
    "    log(\"Source graphs3: \")\n",
    "    log(\"prox_adj3: %d nodes, %d edges\" % (source_prox_adj3.shape[0], np.sum(source_prox_adj3)))\n",
    "    log(\"road adj3: %d nodes, %d edges\" % (source_road_adj3.shape[0], np.sum(source_road_adj3 > 0)))\n",
    "    log(\"poi_adj3, %d nodes, %d edges\" % (source_poi_adj3.shape[0], np.sum(source_poi_adj3 > 0)))\n",
    "    log(\"s_adj3, %d nodes, %d edges\" % (source_s_adj3.shape[0], np.sum(source_s_adj3 > 0)))\n",
    "    log(\"d_adj3, %d nodes, %d edges\" % (source_d_adj3.shape[0], np.sum(source_d_adj3 > 0)))\n",
    "    log()\n",
    "    source_graphs3 = adjs_to_graphs([source_prox_adj3, source_road_adj3, source_poi_adj3, source_s_adj3, source_d_adj3])\n",
    "    for i in range(len(source_graphs3)):\n",
    "        source_graphs3[i] = source_graphs3[i].to(device)\n",
    "    source_edges3, source_edge_labels3 = graphs_to_edge_labels(source_graphs3)\n",
    "\n",
    "if args.need_geo_weight == 1:\n",
    "    log(\"============================\")\n",
    "    log(\"=======use geo score========\")\n",
    "    log(\"============================\")\n",
    "    path2 = \"./geo_weight/geo_weight{}_{}_{}_{}_{}.npy\"\n",
    "    geo_weight1 = np.load(path2.format(scity, tcity, datatype, dataname, args.data_amount))\n",
    "    geo_weight2 = np.load(path2.format(scity2, tcity, datatype, dataname, args.data_amount))\n",
    "    if args.need_third == 1:\n",
    "        geo_weight3 = np.load(path2.format(scity3, tcity, datatype, dataname, args.data_amount))\n",
    "    # c1shape = source_data.shape[1], source_data.shape[2], 14\n",
    "    # c2shape = source_data2.shape[1], source_data2.shape[2], 14\n",
    "    # ctshape = target_data.shape[1], target_data.shape[2], 14\n",
    "    # spoi1 = source_norm_poi.reshape(c1shape)\n",
    "    # spoi2 = source_norm_poi2.reshape(c2shape)\n",
    "    # tpoi = target_norm_poi.reshape(ctshape)\n",
    "    # dis_method = args.geo_dis\n",
    "    # log(\"geo dis meth :{}\".format(dis_method))\n",
    "    # geo_weight1 = calculateGeoSimilarity(spoi1, source_road_adj, source_s_adj, source_t_adj, mask_source, tpoi,\n",
    "    #                                      target_road_adj, target_s_adj, target_t_adj, mask_target, dis_method=dis_method)\n",
    "    # geo_weight2 = calculateGeoSimilarity(spoi2, source_road_adj2, source_s_adj2, source_t_adj2, mask_source2, tpoi,\n",
    "    #                                      target_road_adj, target_s_adj, target_t_adj, mask_target, dis_method=dis_method)\n",
    "    # if args.need_third == 1:\n",
    "    #     c3shape = source_data3.shape[1], source_data3.shape[2], 14\n",
    "    #     spoi3 = source_norm_poi3.reshape(c3shape)\n",
    "    #     geo_weight3 = calculateGeoSimilarity(spoi3, source_road_adj3, source_s_adj3, source_t_adj3, mask_source3, tpoi,\n",
    "    #                                          target_road_adj, target_s_adj, target_t_adj, mask_target, dis_method=dis_method)\n",
    "\n",
    "virtual_city = None\n",
    "virtual_poi = None\n",
    "virtual_road = None\n",
    "virtual_od = None\n",
    "virtual_source_coord = None\n",
    "\n",
    "path = \"./time_weight/time_weight{}_{}_{}_{}_{}.npy\"\n",
    "s1_time_weight = np.load(path.format(scity, tcity, datatype, dataname, args.data_amount)).sum(2)\n",
    "s1_time_weight, _, _ = min_max_normalize(s1_time_weight)\n",
    "\n",
    "s2_time_weight = np.load(path.format(scity2, tcity, datatype, dataname, args.data_amount)).sum(2)\n",
    "s2_time_weight, _, _ = min_max_normalize(s2_time_weight)\n",
    "\n",
    "s1_regions = []\n",
    "s2_regions = []\n",
    "s3_regions = []\n",
    "\n",
    "if args.need_third == 1:\n",
    "    s3_time_weight = np.load(path.format(scity3, tcity, datatype, dataname, args.data_amount)).sum(2)\n",
    "    s3_time_weight, _, _ = min_max_normalize(s3_time_weight)\n",
    "\n",
    "if args.need_geo_weight == 1:\n",
    "    s1_time_weight = args.time_rate * s1_time_weight + args.geo_rate * geo_weight1\n",
    "    s2_time_weight = args.time_rate * s2_time_weight + args.geo_rate * geo_weight2\n",
    "    if args.need_third == 1:\n",
    "        s3_time_weight = args.time_rate * s3_time_weight + args.geo_rate * geo_weight3\n",
    "threshold = args.threshold\n",
    "s1_amont = args.s1_amont\n",
    "s2_amont = args.s2_amont\n",
    "s3_amont = args.s3_amont\n",
    "time_threshold = args.cut_data\n",
    "\n",
    "\n",
    "def dfs(maps, i, j):\n",
    "    \"\"\"\n",
    "\n",
    "    @param maps: two dimension array like\n",
    "    @param i: coord\n",
    "    @param j: coord\n",
    "    \"\"\"\n",
    "    if i < 0 or i >= maps.shape[0] or j < 0 or j >= maps.shape[1] or maps[i][j] == False:\n",
    "        return []\n",
    "    maps[i][j] = False\n",
    "    coord_list = []\n",
    "    coord_list.append((i, j))\n",
    "    for p in [-1, 0, 1]:\n",
    "        for q in [-1, 0, 1]:\n",
    "            if p == q and p == 0:\n",
    "                continue\n",
    "            coord_list.extend(dfs(maps, i + p, j + q))\n",
    "    return coord_list\n",
    "\n",
    "\n",
    "def calculate_linked_regions(t1, need_graph=False, threshold=0.2):\n",
    "    mask_t1 = t1 > threshold\n",
    "    if need_graph:\n",
    "        import seaborn as sns\n",
    "        fig = sns.heatmap(mask_t1)\n",
    "        heatmap = fig.get_figure()\n",
    "        heatmap.show()\n",
    "    # =======================\n",
    "    # 求连通域\n",
    "    # =======================\n",
    "    city_regions = []\n",
    "    count = 0\n",
    "    for i in range(mask_t1.shape[0]):\n",
    "        for j in range(mask_t1.shape[1]):\n",
    "\n",
    "            if mask_t1[i][j]:\n",
    "                coord_list = []\n",
    "                count += 1\n",
    "                coord_list.extend(dfs(mask_t1, i, j))\n",
    "                city_regions.append(coord_list)\n",
    "    log(\"连通域的数量：{}\".format(str(count)))\n",
    "    linked_regions = np.zeros(mask_t1.shape)\n",
    "    for i, x in enumerate(city_regions):\n",
    "        for j in x:\n",
    "            linked_regions[j[0]][j[1]] = i + 1\n",
    "    if need_graph:\n",
    "        fig = sns.heatmap(linked_regions, annot=True)\n",
    "        heatmap = fig.get_figure()\n",
    "        heatmap.show()\n",
    "    # ==================\n",
    "    # 排除重复的内部区域\n",
    "    # ==================\n",
    "    linked_regions_range = []\n",
    "    area_max = (0, 0, 0, 0, 0)\n",
    "    for i in city_regions:\n",
    "        x, y = [], []\n",
    "        for j in i:\n",
    "            x.append(j[0])\n",
    "            y.append(j[1])\n",
    "        x_max = np.max(x)\n",
    "        x_min = np.min(x)\n",
    "        y_max = np.max(y)\n",
    "        y_min = np.min(y)\n",
    "        a = abs(x_max - x_min) * abs(y_max - y_min)\n",
    "        if a > area_max[4]:\n",
    "            area_max = [x_min, x_max, y_min, y_max, a, True]\n",
    "        linked_regions_range.append([x_min, x_max, y_min, y_max, a, True])\n",
    "\n",
    "    for i in linked_regions_range:\n",
    "        if i[0] >= area_max[0] and i[1] <= area_max[1] \\\n",
    "                and i[2] >= area_max[2] and i[3] <= area_max[3] and i[4] <= area_max[4]:\n",
    "            if i == area_max:\n",
    "                continue\n",
    "            i[5] = False\n",
    "\n",
    "    # ================\n",
    "    # 求组件范围\n",
    "    # ================\n",
    "    linked_regions = np.zeros(mask_t1.shape)\n",
    "    ccc = 1\n",
    "    for i in linked_regions_range:\n",
    "        if not i[5]:\n",
    "            continue\n",
    "        for p in range(mask_t1.shape[0]):\n",
    "            for q in range(mask_t1.shape[1]):\n",
    "                if i[0] - 1 <= p <= i[1] + 1 and i[2] - 1 <= q <= i[3] + 1 and i[5] == True:\n",
    "                    linked_regions[p][q] = ccc\n",
    "        ccc += 1\n",
    "    log(\"排除包含关系之后连通域数量：{}\".format(str(ccc - 1)))\n",
    "    if need_graph:\n",
    "        fig = sns.heatmap(linked_regions, annot=True)\n",
    "        heatmap = fig.get_figure()\n",
    "        heatmap.show()\n",
    "    # =======================\n",
    "    # 组合起来\n",
    "    # =======================\n",
    "    boxes = []\n",
    "    coord_range = []\n",
    "    for i in linked_regions_range:\n",
    "        if i[5]:\n",
    "            a, b, c, d = i[0] - 1 if i[0] - 1 > 0 else 0, i[1] + 1 if i[1] + 1 < t1.shape[0] else t1.shape[0] - 1, \\\n",
    "                         i[2] - 1 if i[2] - 1 > 0 else 0, i[3] + 1 if i[3] + 1 < t1.shape[1] else t1.shape[1] - 1\n",
    "            coord_range.append([a, b, c, d,\n",
    "                                (b - a + 1) * (d - c + 1),\n",
    "                                True])\n",
    "            boxes.append([abs(coord_range[-1][1] - coord_range[-1][0]) + 1,\n",
    "                          abs(coord_range[-1][3] - coord_range[-1][2]) + 1])\n",
    "    return boxes, coord_range\n",
    "\n",
    "\n",
    "boxes1, linked_regions_range1 = calculate_linked_regions(s1_time_weight, False, args.s1_rate)\n",
    "boxes2, linked_regions_range2 = calculate_linked_regions(s2_time_weight, False, args.s2_rate)\n",
    "boxes3, linked_regions_range3 = [], []\n",
    "if args.need_third == 1:\n",
    "    boxes3, linked_regions_range3 = calculate_linked_regions(s3_time_weight, False, args.s3_rate)\n",
    "log(boxes1, boxes2, boxes3)\n",
    "log(linked_regions_range1, linked_regions_range2, linked_regions_range3)\n",
    "log([sum(j[4] for j in i) for i in [linked_regions_range1, linked_regions_range2, linked_regions_range3]])\n",
    "\n",
    "from ph import phspprg, phsppog\n",
    "from visualize import visualize\n",
    "from collections import namedtuple\n",
    "\n",
    "Rectangle = namedtuple('Rectangle', ['x', 'y', 'w', 'h'])\n",
    "\n",
    "boxes = []\n",
    "boxes.extend(boxes1)\n",
    "boxes.extend(boxes2)\n",
    "if args.need_third == 1:\n",
    "    boxes.extend(boxes3)\n",
    "sum_area = 0\n",
    "for i in [linked_regions_range1, linked_regions_range2, linked_regions_range3]:\n",
    "    for j in i:\n",
    "        sum_area += j[4]\n",
    "sum_min = 999999999\n",
    "width_min = 0\n",
    "\n",
    "\n",
    "def verify(width, height, rectangles):\n",
    "    for i in rectangles:\n",
    "        if i.x + i.w > width or i.y + i.h > height:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "for i in range(10, (int(math.sqrt(sum_area)) + 1) * 2, 1):\n",
    "    width = i\n",
    "    height, rectangles = phspprg(width, boxes)\n",
    "    if height + width < sum_min and verify(width, height, rectangles):\n",
    "        width_min = i\n",
    "        sum_min = height + width\n",
    "height, rectangles = phspprg(width_min, boxes)\n",
    "# visualize(width_min, height, rectangles)\n",
    "log(\"The width for min height is {}\".format(str(width_min)))\n",
    "log(\"The height is: {}\".format(height))\n",
    "width = int(width_min)\n",
    "height = int(height)\n",
    "\n",
    "virtual_city = np.zeros((time_threshold, width, height))\n",
    "virtual_source_coord = np.zeros((3, width, height))\n",
    "virtual_poi = np.zeros((width, height, 14))\n",
    "virtual_road = np.zeros((width * height, width * height))\n",
    "virtual_od = np.zeros((width * height, width * height))\n",
    "city_regions_expand = []\n",
    "for i in linked_regions_range1:\n",
    "    city_regions_expand.append([i[0], i[1], i[2], i[3], i[4], abs(i[1] - i[0]) + 1, abs(i[3] - i[2]) + 1, 1, False])\n",
    "for i in linked_regions_range2:\n",
    "    city_regions_expand.append([i[0], i[1], i[2], i[3], i[4], abs(i[1] - i[0]) + 1, abs(i[3] - i[2]) + 1, 2, False])\n",
    "for i in linked_regions_range3:\n",
    "    city_regions_expand.append([i[0], i[1], i[2], i[3], i[4], abs(i[1] - i[0]) + 1, abs(i[3] - i[2]) + 1, 3, False])\n",
    "\n",
    "\n",
    "def find_city_regions(w, h):\n",
    "    for i in city_regions_expand:\n",
    "        if not i[-1] and i[5] == w and i[6] == h:\n",
    "            i[-1] = True\n",
    "            return i[-2], i[0], i[1], i[2], i[3]\n",
    "    return None\n",
    "\n",
    "\n",
    "test_mask = np.zeros((width, height))\n",
    "for i in rectangles:\n",
    "    res = None\n",
    "    res = find_city_regions(int(i.w), int(i.h))\n",
    "    across_flag = False\n",
    "    if res is None:\n",
    "        \"\"\"\n",
    "        拼接的时候可能会旋转\n",
    "        \"\"\"\n",
    "        res = find_city_regions(int(i.h), int(i.w))\n",
    "        across_flag = True\n",
    "    data = None\n",
    "    data_poi = None\n",
    "    log(i)\n",
    "    log(res)\n",
    "    log(across_flag)\n",
    "    if res[0] == 1:\n",
    "        data = source_data\n",
    "        data_poi = source_poi\n",
    "    elif res[0] == 2:\n",
    "        data = source_data2\n",
    "        data_poi = source_poi2\n",
    "    elif res[0] == 3:\n",
    "        data = source_data3\n",
    "        data_poi = source_poi3\n",
    "\n",
    "    for p in range(int(i.w)):\n",
    "        for q in range(int(i.h)):\n",
    "            if across_flag:\n",
    "                virtual_city[:, i.x + p, i.y + q] = data[0: time_threshold, res[1] + q, res[3] + p]\n",
    "                virtual_source_coord[:, i.x + p, i.y + q] = np.array([res[1] + q, res[3] + p, res[0]])\n",
    "                data_poi = data_poi.reshape((data.shape[1], data.shape[2], 14))\n",
    "                virtual_poi[i.x + p, i.y + q, :] = data_poi[res[1] + q, res[3] + p, :]\n",
    "                test_mask[i.x + p, i.y + q] = 1\n",
    "            else:\n",
    "                virtual_city[:, i.x + p, i.y + q] = data[0: time_threshold, res[1] + p, res[3] + q]\n",
    "                virtual_source_coord[:, i.x + p, i.y + q] = np.array([res[1] + p, res[3] + q, res[0]])\n",
    "                data_poi = data_poi.reshape((data.shape[1], data.shape[2], 14))\n",
    "                virtual_poi[i.x + p, i.y + q, :] = data_poi[res[1] + p, res[3] + q, :]\n",
    "                test_mask[i.x + p, i.y + q] = 1\n",
    "\n",
    "log()\n",
    "# import seaborn as sns\n",
    "#\n",
    "# fig = sns.heatmap(test_mask)\n",
    "# heatmap = fig.get_figure()\n",
    "# heatmap.show()\n",
    "\n",
    "for i in range(virtual_source_coord.shape[1] * virtual_source_coord.shape[2]):\n",
    "    for j in range(virtual_source_coord.shape[1] * virtual_source_coord.shape[2]):\n",
    "        m, n = idx_1d22d(i, (virtual_source_coord.shape[1], virtual_source_coord.shape[2]))\n",
    "        p, q = idx_1d22d(j, (virtual_source_coord.shape[1], virtual_source_coord.shape[2]))\n",
    "        if virtual_source_coord[2][m][n] == virtual_source_coord[2][p][q]:\n",
    "            od = None\n",
    "            road = None\n",
    "            shape = None\n",
    "            if virtual_source_coord[2][m][n] == 1:\n",
    "                od = source_od_adj\n",
    "                road = source_road_adj\n",
    "                shape = (source_data.shape[1], source_data.shape[2])\n",
    "            elif virtual_source_coord[2][m][n] == 2:\n",
    "                od = source_od_adj2\n",
    "                road = source_road_adj2\n",
    "                shape = (source_data2.shape[1], source_data2.shape[2])\n",
    "            elif virtual_source_coord[2][m][n] == 3:\n",
    "                od = source_od_adj3\n",
    "                road = source_road_adj3\n",
    "                shape = (source_data3.shape[1], source_data3.shape[2])\n",
    "            else:\n",
    "                continue\n",
    "            c = idx_2d_2_1d(\n",
    "                (virtual_source_coord[0][m][n], virtual_source_coord[1][m][n]\n",
    "                 ), shape)\n",
    "            d = idx_2d_2_1d(\n",
    "                (virtual_source_coord[0][p][q], virtual_source_coord[1][p][q]\n",
    "                 ), shape)\n",
    "            c = int(c)\n",
    "            d = int(d)\n",
    "            # log(\"m, n, p, q, c, d\", \" \", m, n, p, q, c, d)\n",
    "            virtual_od[i][j] = od[c][d]\n",
    "            virtual_road[i][j] = road[c][d]\n",
    "for i in range(virtual_road.shape[0]):\n",
    "    virtual_road[i][i] = 1\n",
    "log()\n",
    "\n",
    "long_term_save[\"virtual_source_coord\"] = virtual_source_coord\n",
    "long_term_save[\"virtual_city\"] = virtual_city\n",
    "long_term_save[\"virtual_poi\"] = virtual_poi\n",
    "long_term_save[\"virtual_road\"] = virtual_road\n",
    "long_term_save[\"virtual_od\"] = virtual_od\n",
    "virtual_poi = virtual_poi.reshape((virtual_city.shape[1] * virtual_city.shape[2], 14))\n",
    "lng_virtual, lat_virtual = virtual_city.shape[1], virtual_city.shape[2]\n",
    "mask_virtual = virtual_city.sum(0) > 0\n",
    "th_mask_virtual = torch.Tensor(mask_virtual.reshape(1, lng_virtual, lat_virtual)).to(device)\n",
    "log(\"%d valid regions in virtual\" % np.sum(mask_virtual))\n",
    "virtual_emb_label = masked_percentile_label(virtual_city.sum(0).reshape(-1), mask_virtual.reshape(-1))\n",
    "lag = [-6, -5, -4, -3, -2, -1]\n",
    "# virtual_city, virtual_max, virtual_min = min_max_normalize(virtual_city)\n",
    "virtual_train_x, virtual_train_y, virtual_val_x, virtual_val_y, virtual_test_x, virtual_test_y \\\n",
    "    = split_x_y(virtual_city, lag, val_num=int(virtual_city.shape[0] / 6), test_num=int(virtual_city.shape[0] / 6))\n",
    "# we concatenate all source data\n",
    "virtual_x = np.concatenate([virtual_train_x, virtual_val_x, virtual_test_x], axis=0)\n",
    "virtual_y = np.concatenate([virtual_train_y, virtual_val_y, virtual_test_y], axis=0)\n",
    "virtual_test_dataset = TensorDataset(torch.Tensor(virtual_test_x), torch.Tensor(virtual_test_y))\n",
    "virtual_test_loader = DataLoader(virtual_test_dataset, batch_size=args.batch_size)\n",
    "virtual_dataset = TensorDataset(torch.Tensor(virtual_x), torch.Tensor(virtual_y))\n",
    "virtual_loader = DataLoader(virtual_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "virtual_transform = TfidfTransformer()\n",
    "virtual_norm_poi = np.array(virtual_transform.fit_transform(virtual_poi).todense())\n",
    "virtual_poi_adj, virtual_poi_cos = build_poi_graph(virtual_norm_poi, args.topk)\n",
    "virtual_poi_adj = add_self_loop(virtual_poi_adj)\n",
    "virtual_prox_adj = add_self_loop(build_prox_graph(lng_virtual, lat_virtual))\n",
    "virtual_road_adj = virtual_road\n",
    "d_sim = np.dot(virtual_od, virtual_od.transpose())\n",
    "s_sim = np.dot(virtual_od.transpose(), virtual_od)\n",
    "d_norm = np.sqrt((virtual_od ** 2).sum(1))\n",
    "s_norm = np.sqrt((virtual_od ** 2).sum(0))\n",
    "d_sim /= (np.outer(d_norm, d_norm) + 1e-5)\n",
    "s_sim /= (np.outer(s_norm, s_norm) + 1e-5)\n",
    "s_adj = np.copy(s_sim)\n",
    "d_adj = np.copy(d_sim)\n",
    "n_nodes = s_adj.shape[0]\n",
    "for i in range(n_nodes):\n",
    "    s_adj[i, np.argsort(s_sim[i, :])[:-args.topk]] = 0\n",
    "    s_adj[np.argsort(s_sim[:, i])[:-args.topk], i] = 0\n",
    "    d_adj[i, np.argsort(d_sim[i, :])[:-args.topk]] = 0\n",
    "    d_adj[np.argsort(d_sim[:, i])[:-args.topk], i] = 0\n",
    "virtual_s_adj, virtual_d_adj, virtual_od_adj = s_adj, d_adj, virtual_od\n",
    "virtual_s_adj = add_self_loop(virtual_s_adj)\n",
    "virtual_d_adj = add_self_loop(virtual_d_adj)\n",
    "virtual_od_adj = add_self_loop(virtual_od_adj)\n",
    "log()\n",
    "\n",
    "log(\"virtual graphs: \")\n",
    "log(\"virtual_poi_adj, %d nodes, %d edges\" % (virtual_poi_adj.shape[0], np.sum(virtual_poi_adj > 0)))\n",
    "log(\"prox_adj3: %d nodes, %d edges\" % (virtual_prox_adj.shape[0], np.sum(virtual_prox_adj)))\n",
    "log(\"road adj3: %d nodes, %d edges\" % (virtual_road_adj.shape[0], np.sum(virtual_road_adj > 0)))\n",
    "log(\"s_adj3, %d nodes, %d edges\" % (virtual_s_adj.shape[0], np.sum(virtual_s_adj > 0)))\n",
    "log(\"d_adj3, %d nodes, %d edges\" % (virtual_d_adj.shape[0], np.sum(virtual_d_adj > 0)))\n",
    "log()\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.device) if torch.cuda.is_available() else \"cpu\")\n",
    "log(f'device: {device}')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "if args.labelrate > 100:\n",
    "    args.labelrate = 100\n",
    "\n",
    "adj_pems04, adj_pems07, adj_pems08 = load_all_adj(device)\n",
    "vec_pems04 = vec_pems07 = vec_pems08 = None, None, None\n",
    "virtual_road = np.where(virtual_road >= 1, 1, virtual_road)\n",
    "virtual_road = add_self_loop(virtual_road)\n",
    "for m in range(virtual_road.shape[0]):\n",
    "    for n in range(virtual_road.shape[1]):\n",
    "        a, b = idx_1d22d(m, virtual_road.shape)\n",
    "        c, d = idx_1d22d(n, virtual_road.shape)\n",
    "        dis = abs(a - c) + abs(b - d)\n",
    "        if virtual_road[m][n] - 0 > 1e-6 and dis != 0:\n",
    "            virtual_road[m][n] = virtual_road[m][n] / dis\n",
    "adj_virtual = torch.tensor(virtual_road).to(device)\n",
    "dc = np.load(\"./data/DC/{}DC_{}.npy\".format(args.dataname, args.datatype))\n",
    "dcmask = dc.sum(0) > 0\n",
    "\n",
    "chi = np.load(\"./data/CHI/{}CHI_{}.npy\".format(args.dataname, args.datatype))\n",
    "chimask = chi.sum(0) > 0\n",
    "\n",
    "ny = np.load(\"./data/NY/{}NY_{}.npy\".format(args.dataname, args.datatype))\n",
    "nymask = ny.sum(0) > 0\n",
    "\n",
    "target_road_adj = np.where(target_road_adj >= 1, 1, target_road_adj)\n",
    "target_road_adj = add_self_loop(target_road_adj)\n",
    "for m in range(target_road_adj.shape[0]):\n",
    "    for n in range(target_road_adj.shape[1]):\n",
    "        a, b = idx_1d22d(m, target_road_adj.shape)\n",
    "        c, d = idx_1d22d(n, target_road_adj.shape)\n",
    "        dis = abs(a - c) + abs(b - d)\n",
    "        if target_road_adj[m][n] - 0 > 1e-6 and dis != 0:\n",
    "            target_road_adj[m][n] = target_road_adj[m][n] / dis\n",
    "if args.need_road_adj:\n",
    "    target_graphs = adjs_to_graphs([target_prox_adj, target_road_adj, target_poi_adj, target_s_adj, target_d_adj])\n",
    "\n",
    "    virtual_graphs = adjs_to_graphs([virtual_prox_adj, virtual_road, virtual_poi_adj, virtual_s_adj, virtual_d_adj])\n",
    "else:\n",
    "    target_graphs = adjs_to_graphs([target_prox_adj, target_poi_adj, target_s_adj, target_d_adj])\n",
    "\n",
    "    virtual_graphs = adjs_to_graphs([virtual_prox_adj, virtual_poi_adj, virtual_s_adj, virtual_d_adj])\n",
    "\n",
    "for i in range(len(virtual_graphs)):\n",
    "    virtual_graphs[i] = virtual_graphs[i].to(device)\n",
    "    target_graphs[i] = target_graphs[i].to(device)\n",
    "virtual_edges, virtual_edge_labels = graphs_to_edge_labels(virtual_graphs)\n",
    "target_edges, target_edge_labels = graphs_to_edge_labels(target_graphs)\n",
    "\n",
    "\n",
    "class Scoring(nn.Module):\n",
    "    def __init__(self, emb_dim, source_mask, target_mask):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.score = nn.Sequential(nn.Linear(self.emb_dim, self.emb_dim // 2),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Linear(self.emb_dim // 2, self.emb_dim // 2))\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask\n",
    "\n",
    "    def forward(self, source_emb, target_emb, source_mask, target_mask):\n",
    "        \"\"\"\n",
    "        求源城市评分\n",
    "        注意这里求评分，是source的每一个区域对于目标城市整体\n",
    "        换句话说，是形参2的每一个区域，对于形参3整体\n",
    "        :param target_mask:\n",
    "        :param source_mask:\n",
    "        :param source_emb:\n",
    "        :param target_emb:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # target_context = tanh(self.score(target_emb[bool mask]).mean(0))\n",
    "        # 对于横向的进行求平均 460*64 -> 460*32 -> 207*32 -> 纵向求平均 1*32 代表所有目标城市\n",
    "        target_context = torch.tanh(self.score(target_emb[target_mask.view(-1).bool()]).mean(0))\n",
    "        source_trans_emb = self.score(source_emb)\n",
    "        source_score = (source_trans_emb * target_context).sum(1)\n",
    "        return F.relu(torch.tanh(source_score))[source_mask.view(-1).bool()]\n",
    "\n",
    "\n",
    "cross_num_gat_layers = 2\n",
    "cross_in_dim = 14\n",
    "cross_hidden_dim = 64\n",
    "cross_emb_dim = 64\n",
    "cross_num_heads = 2\n",
    "cross_mmd_w = args.mmd_w\n",
    "cross_et_w = args.et_w\n",
    "cross_ma_param = args.ma_coef\n",
    "mvgat = MVGAT(len(virtual_graphs), cross_num_gat_layers, cross_in_dim, cross_hidden_dim, cross_emb_dim, cross_num_heads,\n",
    "              True).to(device)\n",
    "fusion = FusionModule(len(virtual_graphs), cross_emb_dim, 0.8).to(device)\n",
    "scoring = Scoring(cross_emb_dim, th_mask_virtual, th_mask_target).to(device)\n",
    "edge_disc = EdgeTypeDiscriminator(len(virtual_graphs), cross_emb_dim).to(device)\n",
    "mmd = MMD_loss()\n",
    "emb_param_list = list(mvgat.parameters()) + list(fusion.parameters()) + list(edge_disc.parameters())\n",
    "emb_optimizer = optim.Adam(emb_param_list, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "mvgat_optimizer = optim.Adam(list(mvgat.parameters()) + list(fusion.parameters()), lr=args.learning_rate,\n",
    "                             weight_decay=args.weight_decay)\n",
    "\n",
    "meta_optimizer = optim.Adam(scoring.parameters(), lr=args.outerlr, weight_decay=args.weight_decay)\n",
    "best_val_rmse = 999\n",
    "best_test_rmse = 999\n",
    "best_test_mae = 999\n",
    "best_test_mape = 999\n",
    "p_bar.process(5, 1, 5)\n",
    "\n",
    "\n",
    "class DomainClassify(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dc = nn.Sequential(nn.Linear(self.emb_dim, self.emb_dim // 2),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Linear(self.emb_dim // 2, self.emb_dim // 2),\n",
    "                                nn.Linear(self.emb_dim // 2, 2))\n",
    "\n",
    "    def forward(self, feature):\n",
    "        res = torch.sigmoid(self.dc(feature))\n",
    "        return res\n",
    "\n",
    "\n",
    "def forward_emb(graphs_, in_feat_, od_adj_, poi_cos_):\n",
    "    \"\"\"\n",
    "    1. 图卷积提取图特征 mvgat\n",
    "    2. 融合多图特征 fusion\n",
    "    3. 对于多图中的s，d，poi进行预测，并计算损失函数\n",
    "    :param graphs_:\n",
    "    :param in_feat_:\n",
    "    :param od_adj_:\n",
    "    :param poi_cos_:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 图注意，注意这里用了小写，指的是forward方法\n",
    "    views = mvgat(graphs_, torch.Tensor(in_feat_).to(device))\n",
    "    fused_emb, embs = fusion(views)\n",
    "    # embs嵌入是5个图，以下找出start，destination， poi图\n",
    "    s_emb = embs[-2]\n",
    "    d_emb = embs[-1]\n",
    "    poi_emb = embs[-3]\n",
    "    # start和destination相乘求出记录预测s和d\n",
    "    recons_sd = torch.matmul(s_emb, d_emb.transpose(0, 1))\n",
    "    # 注意dim维度0和1分别求s和d\n",
    "    pred_d = torch.log(torch.softmax(recons_sd, dim=1) + 1e-5)\n",
    "    loss_d = (torch.Tensor(od_adj_).to(device) * pred_d).mean()\n",
    "    pred_s = torch.log(torch.softmax(recons_sd, dim=0) + 1e-5)\n",
    "    loss_s = (torch.Tensor(od_adj_).to(device) * pred_s).mean()\n",
    "    # poi预测求差，loss\n",
    "    poi_sim = torch.matmul(poi_emb, poi_emb.transpose(0, 1))\n",
    "    loss_poi = ((poi_sim - torch.Tensor(poi_cos_).to(device)) ** 2).mean()\n",
    "    loss = -loss_s - loss_d + loss_poi\n",
    "\n",
    "    return loss, fused_emb, embs\n",
    "\n",
    "\n",
    "if args.node_adapt == \"DT\":\n",
    "    # ============================================================================================\n",
    "    # 预训练特征提取网络mvgat， 方便训练域识别网络\n",
    "    # ============================================================================================\n",
    "    loss_mvgats = []\n",
    "    # 实验确定\n",
    "    pre = 25\n",
    "    for i in range(pre):\n",
    "        loss_source, fused_emb_s, embs_s = forward_emb(virtual_graphs, virtual_norm_poi, virtual_od_adj,\n",
    "                                                       virtual_poi_cos)\n",
    "        loss_target, fused_emb_t, embs_t = forward_emb(target_graphs, target_norm_poi, target_od_adj, target_poi_cos)\n",
    "\n",
    "        loss_mvgat = loss_source + loss_target\n",
    "        meta_optimizer.zero_grad()\n",
    "        loss_mvgat.backward()\n",
    "        emb_optimizer.step()\n",
    "        loss_mvgats.append(loss_mvgat.item())\n",
    "    #     log(\"loss_mvgat:{}\".format(str(loss_mvgat)))\n",
    "    # loss_mvgats = np.array(loss_mvgats)\n",
    "    # x = np.array([i + 1 for i in range(pre)])\n",
    "    # plt.plot(x, loss_mvgats)\n",
    "    # plt.grid()\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        views = mvgat(virtual_graphs, torch.Tensor(virtual_norm_poi).to(device))\n",
    "        # 融合模块指的是把多图的特征融合\n",
    "        fused_emb_s, _ = fusion(views)\n",
    "        views = mvgat(target_graphs, torch.Tensor(target_norm_poi).to(device))\n",
    "        fused_emb_t, _ = fusion(views)\n",
    "\n",
    "    s1 = np.array([1, 0])\n",
    "    st = np.array([0, 1])\n",
    "    x = torch.concat((fused_emb_s[th_mask_virtual.view(-1).bool()],\n",
    "                      fused_emb_t[th_mask_target.view(-1).bool()]), dim=0)\n",
    "    y = []\n",
    "    y.extend([s1 for i in range(fused_emb_s[th_mask_virtual.view(-1).bool()].shape[0])])\n",
    "    y.extend([st for i in range(fused_emb_t[th_mask_target.view(-1).bool()].shape[0])])\n",
    "    y = torch.from_numpy(np.array(y))\n",
    "    x = x.cpu().numpy()\n",
    "    y = y.numpy()\n",
    "    random_ids = np.random.randint(0, x.shape[0], size=x.shape[0])\n",
    "    x = x[random_ids]\n",
    "    y = y[random_ids]\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    dt_train = (x[0: 400], y[0: 400])\n",
    "    dt_val = (x[400: 600], y[400: 600])\n",
    "    dt_test = (x[600:], y[600:])\n",
    "    dt_train_dataset = TensorDataset(dt_train[0], dt_train[1])\n",
    "    dt_val_dataset = TensorDataset(dt_val[0], dt_val[1])\n",
    "    dt_test_dataset = TensorDataset(dt_test[0], dt_test[1])\n",
    "    dt_train_loader = DataLoader(dt_train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    dt_val_loader = DataLoader(dt_val_dataset, batch_size=args.batch_size)\n",
    "    dt_test_loader = DataLoader(dt_test_dataset, batch_size=args.batch_size)\n",
    "    dt = DomainClassify(emb_dim=cross_emb_dim)\n",
    "    dt.to(device)\n",
    "    dt_optimizer = optim.Adam(dt.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "    dc_epoch = 10\n",
    "    epoch_loss = []\n",
    "    val_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    for i in range(dc_epoch):\n",
    "        temp = []\n",
    "        dt.train()\n",
    "        for i, (x, y) in enumerate(dt_train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = dt(x)\n",
    "            loss = ((out - y) ** 2)\n",
    "            loss = loss.sum()\n",
    "            dt_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(dt.parameters(), max_norm=2)\n",
    "            dt_optimizer.step()\n",
    "            temp.append(loss.item())\n",
    "        epoch_loss.append(np.array(temp).mean())\n",
    "        dt.eval()\n",
    "        temp = []\n",
    "        for i, (x, y) in enumerate(dt_val_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = dt(x)\n",
    "            loss = ((out - y) ** 2)\n",
    "            loss = loss.sum()\n",
    "            temp.append(loss.item())\n",
    "        val_loss.append(np.array(temp).mean())\n",
    "        temp = []\n",
    "        for i, (x, y) in enumerate(dt_test_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = dt(x)\n",
    "            loss = ((out - y) ** 2)\n",
    "            loss = loss.sum()\n",
    "            temp.append(loss.item())\n",
    "        test_loss.append(np.array(temp).mean())\n",
    "        count_sum = 0\n",
    "        count_true = 0\n",
    "        for i, (x, y) in enumerate(dt_test_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = dt(x)\n",
    "            for i in range(out.shape[0]):\n",
    "                xx = out[i]\n",
    "                yy = y[i]\n",
    "                count_sum = count_sum + 1\n",
    "                xxx = xx.argmax()\n",
    "                yyy = yy.argmax()\n",
    "                if xxx.item() == yyy.item():\n",
    "                    count_true = count_true + 1\n",
    "        test_accuracy.append(count_true / count_sum)\n",
    "\n",
    "    #     log((epoch_loss[-1], val_loss[-1], test_loss[-1], test_accuracy[-1]))\n",
    "    # plt.plot(np.array([i + 1 for i in range(dc_epoch)]), np.array(epoch_loss), label=\"train\")\n",
    "    # plt.plot(np.array([i + 1 for i in range(dc_epoch)]), np.array(val_loss), label=\"val\")\n",
    "    # plt.plot(np.array([i + 1 for i in range(dc_epoch)]), np.array(test_loss), label=\"test\")\n",
    "    # plt.plot(np.array([i + 1 for i in range(dc_epoch)]), np.array(test_accuracy), label=\"acc\")\n",
    "    # plt.xlabel(\"epoch\")\n",
    "    # plt.ylabel(\"loss\")\n",
    "    # plt.grid()\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    log(\"============================\")\n",
    "    log(\"训练DT网络结束\")\n",
    "    log(\"============================\")\n",
    "\n",
    "\n",
    "def train_emb_epoch2():\n",
    "    # loss， 460*64， 5*460*64\n",
    "    loss_source, fused_emb_s, embs_s = forward_emb(virtual_graphs, virtual_norm_poi, virtual_od_adj, virtual_poi_cos)\n",
    "    loss_target, fused_emb_t, embs_t = forward_emb(target_graphs, target_norm_poi, target_od_adj, target_poi_cos)\n",
    "\n",
    "    loss_emb = loss_source + loss_target\n",
    "    mmd_losses = None\n",
    "    if args.node_adapt == \"MMD\":\n",
    "        # compute domain adaptation loss\n",
    "        # 随机抽样128个，计算最大平均误差\n",
    "        source_ids = np.random.randint(0, np.sum(mask_virtual), size=(128,))\n",
    "        target_ids = np.random.randint(0, np.sum(mask_target), size=(128,))\n",
    "        # source1 & target\n",
    "        mmd_loss = mmd(fused_emb_s[th_mask_virtual.view(-1).bool()][source_ids, :],\n",
    "                       fused_emb_t[th_mask_target.view(-1).bool()][target_ids, :])\n",
    "\n",
    "        mmd_losses = mmd_loss\n",
    "    elif args.node_adapt == \"DT\":\n",
    "        mmd_losses = dt(fused_emb_s[th_mask_virtual.view(-1).bool()]).sum() + \\\n",
    "                     dt(fused_emb_t[th_mask_target.view(-1).bool()]).sum()\n",
    "\n",
    "    # 随机抽样边256\n",
    "    source_batch_edges = np.random.randint(0, len(virtual_edges), size=(256,))\n",
    "    target_batch_edges = np.random.randint(0, len(target_edges), size=(256,))\n",
    "    source_batch_src = torch.Tensor(virtual_edges[source_batch_edges, 0]).long()\n",
    "    source_batch_dst = torch.Tensor(virtual_edges[source_batch_edges, 1]).long()\n",
    "    source_emb_src = fused_emb_s[source_batch_src, :]\n",
    "    source_emb_dst = fused_emb_s[source_batch_dst, :]\n",
    "    target_batch_src = torch.Tensor(target_edges[target_batch_edges, 0]).long()\n",
    "    target_batch_dst = torch.Tensor(target_edges[target_batch_edges, 1]).long()\n",
    "    target_emb_src = fused_emb_t[target_batch_src, :]\n",
    "    target_emb_dst = fused_emb_t[target_batch_dst, :]\n",
    "    # 源城市目的城市使用同样的边分类器\n",
    "    pred_source = edge_disc.forward(source_emb_src, source_emb_dst)\n",
    "    pred_target = edge_disc.forward(target_emb_src, target_emb_dst)\n",
    "    source_batch_labels = torch.Tensor(virtual_edge_labels[source_batch_edges]).to(device)\n",
    "    target_batch_labels = torch.Tensor(target_edge_labels[target_batch_edges]).to(device)\n",
    "    # -（label*log(sigmod(pred)+0.000001)) + (1-label)*log(1-sigmod+0.000001) sum mean\n",
    "    loss_et_source = -((source_batch_labels * torch.log(torch.sigmoid(pred_source) + 1e-6)) + (\n",
    "            1 - source_batch_labels) * torch.log(1 - torch.sigmoid(pred_source) + 1e-6)).sum(1).mean()\n",
    "    loss_et_target = -((target_batch_labels * torch.log(torch.sigmoid(pred_target) + 1e-6)) + (\n",
    "            1 - target_batch_labels) * torch.log(1 - torch.sigmoid(pred_target) + 1e-6)).sum(1).mean()\n",
    "    loss_et = loss_et_source + loss_et_target\n",
    "\n",
    "    emb_optimizer.zero_grad()\n",
    "    # 公式11\n",
    "    loss = None\n",
    "    if args.node_adapt == \"MMD\":\n",
    "        loss = loss_emb + cross_mmd_w * mmd_losses + cross_et_w * loss_et\n",
    "    elif args.node_adapt == \"DT\":\n",
    "        loss = loss_emb - cross_mmd_w * mmd_losses + cross_et_w * loss_et\n",
    "    loss.backward()\n",
    "    emb_optimizer.step()\n",
    "    return loss_emb.item(), mmd_losses.item(), loss_et.item()\n",
    "\n",
    "\n",
    "emb_losses = []\n",
    "mmd_losses = []\n",
    "edge_losses = []\n",
    "pretrain_emb_epoch = 80\n",
    "# 预训练图数据嵌入，边类型分类，节点对齐 ——> 获得区域特征\n",
    "for emb_ep in range(pretrain_emb_epoch):\n",
    "    loss_emb_, loss_mmd_, loss_et_ = train_emb_epoch2()\n",
    "    emb_losses.append(loss_emb_)\n",
    "    mmd_losses.append(loss_mmd_)\n",
    "    edge_losses.append(loss_et_)\n",
    "log(\"[%.2fs]Pretrain embeddings for %d epochs, average emb loss %.4f, node loss %.4f, edge loss %.4f\" % (\n",
    "    time.time() - start_time, pretrain_emb_epoch, np.mean(emb_losses), np.mean(mmd_losses), np.mean(edge_losses)))\n",
    "with torch.no_grad():\n",
    "    views = mvgat(virtual_graphs, torch.Tensor(virtual_norm_poi).to(device))\n",
    "    # 融合模块指的是把多图的特征融合\n",
    "    fused_emb_s, _ = fusion(views)\n",
    "    views = mvgat(target_graphs, torch.Tensor(target_norm_poi).to(device))\n",
    "    fused_emb_t, _ = fusion(views)\n",
    "\n",
    "long_term_save[\"emb_losses\"] = emb_losses\n",
    "long_term_save[\"mmd_losses\"] = mmd_losses\n",
    "long_term_save[\"edge_losses\"] = edge_losses\n",
    "\n",
    "emb_s = fused_emb_s.cpu().numpy()[mask_virtual.reshape(-1)]\n",
    "emb_t = fused_emb_t.cpu().numpy()[mask_target.reshape(-1)]\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "cvscore_s = cross_validate(logreg, emb_s, virtual_emb_label)['test_score'].mean()\n",
    "cvscore_t = cross_validate(logreg, emb_t, target_emb_label)['test_score'].mean()\n",
    "log(\"[%.2fs]Pretraining embedding, source cvscore %.4f, target cvscore %.4f\" % \\\n",
    "    (time.time() - start_time, cvscore_s, cvscore_t))\n",
    "log()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def net_fix(source, y, weight, mask, fast_weights, bn_vars, net, epoch):\n",
    "    pred_source, shared_pems04_feat, shared_pems07_feat, shared_pems08_feat = net.functional_forward(vec_pems04,\n",
    "                                                                                                     vec_pems07,\n",
    "                                                                                                     vec_pems08,\n",
    "                                                                                                     source,\n",
    "                                                                                                     False,\n",
    "                                                                                                     fast_weights,\n",
    "                                                                                                     bn_vars,\n",
    "                                                                                                     bn_training=True,\n",
    "                                                                                                     data_set=\"4\")\n",
    "    Reverse = True\n",
    "    p = float(epoch) / args.epoch\n",
    "    constant = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "    pems04_pred = domain_classifier(shared_pems04_feat, constant, Reverse)\n",
    "    pems07_pred = domain_classifier(shared_pems07_feat, constant, Reverse)\n",
    "    pems08_pred = domain_classifier(shared_pems08_feat, constant, Reverse)\n",
    "\n",
    "    pems04_label = 0 * torch.ones(pems04_pred.shape[0]).long().to(device)\n",
    "    pems07_label = 1 * torch.ones(pems07_pred.shape[0]).long().to(device)\n",
    "    pems08_label = 2 * torch.ones(pems08_pred.shape[0]).long().to(device)\n",
    "\n",
    "    pems04_pred_label = pems04_pred.max(1, keepdim=True)[1]\n",
    "    pems04_correct = pems04_pred_label.eq(pems04_label.view_as(pems04_pred_label)).sum()\n",
    "    pems07_pred_label = pems07_pred.max(1, keepdim=True)[1]\n",
    "    pems07_correct = pems07_pred_label.eq(pems07_label.view_as(pems07_pred_label)).sum()\n",
    "    pems08_pred_label = pems08_pred.max(1, keepdim=True)[1]\n",
    "    pems08_correct = pems08_pred_label.eq(pems08_label.view_as(pems08_pred_label)).sum()\n",
    "    mmmm = (th_mask_virtual.reshape((-1)))\n",
    "\n",
    "    pems04_pred = pems04_pred[mmmm.bool(), :]\n",
    "    pems04_label = pems04_label[mmmm.bool()]\n",
    "\n",
    "    pems04_loss = F.nll_loss(pems04_pred, pems04_label)\n",
    "    pems07_loss = F.nll_loss(pems07_pred, pems07_label)\n",
    "\n",
    "    mmmm = (th_mask_target.reshape((-1)))\n",
    "    pems08_pred = pems08_pred[mmmm.bool(), :]\n",
    "    pems08_label = pems08_label[mmmm.bool()]\n",
    "    pems08_loss = domain_criterion(pems08_pred, pems08_label)\n",
    "\n",
    "    domain_loss = pems04_loss + pems08_loss\n",
    "    label = y.reshape((pred_source.shape[0], -1, pred_source.shape[2]))\n",
    "\n",
    "    mae_train, rmse_train, mape_train = masked_loss(pred_source, label, maskp=mask_virtual, weight=weight)\n",
    "    fast_loss = mae_train + args.beta * (args.theta * domain_loss)\n",
    "    a = [(i, torch.autograd.grad(fast_loss, fast_weights[i], create_graph=True, allow_unused=True)) for i in\n",
    "         fast_weights.keys()]\n",
    "    grads = {}\n",
    "    used_fast_weight = OrderedDict()\n",
    "    for i in a:\n",
    "        if i[1][0] is not None:\n",
    "            grads[i[0]] = i[1][0]\n",
    "            used_fast_weight[i[0]] = fast_weights[i[0]]\n",
    "\n",
    "    for name, grad in zip(grads.keys(), grads.values()):\n",
    "        fast_weights[name] = fast_weights[name] - args.innerlr * grad\n",
    "    return fast_loss, fast_weights, bn_vars\n",
    "\n",
    "\n",
    "def meta_train_epoch(s_embs, t_embs, net, epoch):\n",
    "    meta_query_losses = []\n",
    "    for meta_ep in range(args.outeriter):\n",
    "        fast_losses = []\n",
    "        fast_weights, bn_vars = get_weights_bn_vars(net)\n",
    "        source_weights = scoring(s_embs, t_embs, th_mask_virtual, th_mask_target)\n",
    "        # inner loop on source, pre-train with weights\n",
    "        for meta_it in range(args.sinneriter):\n",
    "            s_x1, s_y1 = batch_sampler((torch.Tensor(virtual_train_x), torch.Tensor(virtual_train_y)),\n",
    "                                       args.batch_size)\n",
    "            s_x1 = s_x1.reshape((s_x1.shape[0], s_x1.shape[1], s_x1.shape[2] * s_x1.shape[3]))\n",
    "            s_y1 = s_y1.reshape((s_y1.shape[0], s_y1.shape[1], s_y1.shape[2] * s_y1.shape[3]))\n",
    "            s_x1 = s_x1.to(device)\n",
    "            s_y1 = s_y1.to(device)\n",
    "            fast_loss, fast_weights, bn_vars = net_fix(s_x1, s_y1, source_weights, th_mask_virtual, fast_weights,\n",
    "                                                       bn_vars, net, epoch)\n",
    "            fast_losses.append(fast_loss.item())\n",
    "\n",
    "        for meta_it in range(args.tinneriter):\n",
    "            t_x, t_y = batch_sampler((torch.Tensor(target_train_x), torch.Tensor(target_train_y)), args.batch_size)\n",
    "            t_x = t_x.reshape((t_x.shape[0], t_x.shape[1], t_x.shape[2] * t_x.shape[3]))\n",
    "            t_y = t_y.reshape((t_y.shape[0], t_y.shape[1], t_y.shape[2] * t_y.shape[3]))\n",
    "\n",
    "            t_x = t_x.to(device)\n",
    "            t_y = t_y.to(device)\n",
    "            pred_source, shared_pems04_feat, shared_pems07_feat, shared_pems08_feat = net.functional_forward(vec_pems04,\n",
    "                                                                                                             vec_pems07,\n",
    "                                                                                                             vec_pems08,\n",
    "                                                                                                             t_x,\n",
    "                                                                                                             False,\n",
    "                                                                                                             fast_weights,\n",
    "                                                                                                             bn_vars,\n",
    "                                                                                                             bn_training=True,\n",
    "                                                                                                             data_set=\"8\")\n",
    "\n",
    "            Reverse = True\n",
    "            p = float(epoch) / args.epoch\n",
    "            constant = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            pems04_pred = domain_classifier(shared_pems04_feat, constant, Reverse)\n",
    "            pems07_pred = domain_classifier(shared_pems07_feat, constant, Reverse)\n",
    "            pems08_pred = domain_classifier(shared_pems08_feat, constant, Reverse)\n",
    "\n",
    "            pems04_label = 0 * torch.ones(pems04_pred.shape[0]).long().to(device)\n",
    "            pems07_label = 1 * torch.ones(pems07_pred.shape[0]).long().to(device)\n",
    "            pems08_label = 2 * torch.ones(pems08_pred.shape[0]).long().to(device)\n",
    "\n",
    "            pems04_pred_label = pems04_pred.max(1, keepdim=True)[1]\n",
    "            pems04_correct = pems04_pred_label.eq(pems04_label.view_as(pems04_pred_label)).sum()\n",
    "            pems07_pred_label = pems07_pred.max(1, keepdim=True)[1]\n",
    "            pems07_correct = pems07_pred_label.eq(pems07_label.view_as(pems07_pred_label)).sum()\n",
    "            pems08_pred_label = pems08_pred.max(1, keepdim=True)[1]\n",
    "            pems08_correct = pems08_pred_label.eq(pems08_label.view_as(pems08_pred_label)).sum()\n",
    "\n",
    "            mmmm = (th_mask_virtual.reshape((-1)))\n",
    "            pems04_pred = pems04_pred[mmmm.bool(), :]\n",
    "            pems04_label = pems04_label[mmmm.bool()]\n",
    "\n",
    "            pems04_loss = domain_criterion(pems04_pred, pems04_label)\n",
    "            pems07_loss = domain_criterion(pems07_pred, pems07_label)\n",
    "\n",
    "            mmmm = (th_mask_target.reshape((-1)))\n",
    "            pems08_pred = pems08_pred[mmmm.bool(), :]\n",
    "            pems08_label = pems08_label[mmmm.bool()]\n",
    "\n",
    "            pems08_loss = domain_criterion(pems08_pred, pems08_label)\n",
    "\n",
    "            domain_loss = pems04_loss + pems08_loss\n",
    "            label = t_y.reshape((pred_source.shape[0], -1, pred_source.shape[2]))\n",
    "            mask = th_mask_target\n",
    "            mae_train, rmse_train, mape_train = masked_loss(pred_source, label, maskp=mask_target)\n",
    "            fast_loss = mae_train + args.beta * (args.theta * domain_loss)\n",
    "            a = [(i, torch.autograd.grad(fast_loss, fast_weights[i], create_graph=True, allow_unused=True)) for i in\n",
    "                 fast_weights.keys()]\n",
    "            grads = {}\n",
    "            used_fast_weight = OrderedDict()\n",
    "            for i in a:\n",
    "                if i[1][0] is not None:\n",
    "                    grads[i[0]] = i[1][0]\n",
    "                    used_fast_weight[i[0]] = fast_weights[i[0]]\n",
    "\n",
    "            for name, grad in zip(grads.keys(), grads.values()):\n",
    "                fast_weights[name] = fast_weights[name] - args.innerlr * grad\n",
    "\n",
    "        q_losses = []\n",
    "        target_iter = max(args.sinneriter, args.tinneriter)\n",
    "        for k in range(3):\n",
    "            # query loss\n",
    "            x_q = None\n",
    "            y_q = None\n",
    "            temp_mask = None\n",
    "\n",
    "            x_q, y_q = batch_sampler((torch.Tensor(target_train_x), torch.Tensor(target_train_y)), args.batch_size)\n",
    "            temp_mask = th_mask_target\n",
    "            x_q = x_q.reshape((x_q.shape[0], x_q.shape[1], x_q.shape[2] * x_q.shape[3]))\n",
    "            y_q = y_q.reshape((y_q.shape[0], y_q.shape[1], y_q.shape[2] * y_q.shape[3]))\n",
    "\n",
    "            x_q = x_q.to(device)\n",
    "            y_q = y_q.to(device)\n",
    "            pred_source, shared_pems04_feat, shared_pems07_feat, shared_pems08_feat = \\\n",
    "                net.functional_forward(vec_pems04,\n",
    "                                       vec_pems07,\n",
    "                                       vec_pems08,\n",
    "                                       x_q, False,\n",
    "                                       fast_weights,\n",
    "                                       bn_vars,\n",
    "                                       bn_training=True,\n",
    "                                       data_set=\"8\")\n",
    "            Reverse = True\n",
    "            p = float(epoch) / args.epoch\n",
    "            constant = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            pems04_pred = domain_classifier(shared_pems04_feat, constant, Reverse)\n",
    "            pems07_pred = domain_classifier(shared_pems07_feat, constant, Reverse)\n",
    "            pems08_pred = domain_classifier(shared_pems08_feat, constant, Reverse)\n",
    "\n",
    "            pems04_label = 0 * torch.ones(pems04_pred.shape[0]).long().to(device)\n",
    "            pems07_label = 1 * torch.ones(pems07_pred.shape[0]).long().to(device)\n",
    "            pems08_label = 2 * torch.ones(pems08_pred.shape[0]).long().to(device)\n",
    "\n",
    "            pems04_pred_label = pems04_pred.max(1, keepdim=True)[1]\n",
    "            pems04_correct = pems04_pred_label.eq(pems04_label.view_as(pems04_pred_label)).sum()\n",
    "            pems07_pred_label = pems07_pred.max(1, keepdim=True)[1]\n",
    "            pems07_correct = pems07_pred_label.eq(pems07_label.view_as(pems07_pred_label)).sum()\n",
    "            pems08_pred_label = pems08_pred.max(1, keepdim=True)[1]\n",
    "            pems08_correct = pems08_pred_label.eq(pems08_label.view_as(pems08_pred_label)).sum()\n",
    "            mmmm = (th_mask_virtual.reshape((-1)))\n",
    "            pems04_pred = pems04_pred[mmmm.bool(), :]\n",
    "            pems04_label = pems04_label[mmmm.bool()]\n",
    "\n",
    "            pems04_loss = domain_criterion(pems04_pred, pems04_label)\n",
    "            pems07_loss = domain_criterion(pems07_pred, pems07_label)\n",
    "\n",
    "            mmmm = (th_mask_target.reshape((-1)))\n",
    "            pems08_pred = pems08_pred[mmmm.bool(), :]\n",
    "            pems08_label = pems08_label[mmmm.bool()]\n",
    "\n",
    "            pems08_loss = domain_criterion(pems08_pred, pems08_label)\n",
    "\n",
    "            domain_loss = pems04_loss + pems08_loss\n",
    "            label = y_q.reshape((pred_source.shape[0], -1, pred_source.shape[2]))\n",
    "            mask = temp_mask.reshape((1, temp_mask.shape[1] * temp_mask.shape[2], 1))\n",
    "            mae_train, rmse_train, mape_train = masked_loss(pred_source, label, maskp=mask_target)\n",
    "            fast_loss = mae_train + args.beta * (args.theta * domain_loss)\n",
    "            q_losses.append(fast_loss)\n",
    "        q_loss = torch.stack(q_losses).mean()\n",
    "        weights_mean = source_weights.mean()\n",
    "        meta_loss = q_loss + weights_mean * args.weight_reg\n",
    "        meta_optimizer.zero_grad()\n",
    "        meta_loss.backward(inputs=list(scoring.parameters()), retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(scoring.parameters(), max_norm=2)\n",
    "        meta_optimizer.step()\n",
    "        meta_query_losses.append(q_loss.item())\n",
    "    return np.mean(meta_query_losses)\n",
    "\n",
    "\n",
    "def get_weight(net, type, epoch):\n",
    "    if type == \"fine-tune\":\n",
    "        return None\n",
    "    for emb_ep in range(5):\n",
    "        loss_emb_, loss_mmd_, loss_et_ = train_emb_epoch2()\n",
    "        emb_losses.append(loss_emb_)\n",
    "        mmd_losses.append(loss_mmd_)\n",
    "        edge_losses.append(loss_et_)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        views = mvgat(virtual_graphs, torch.Tensor(virtual_norm_poi).to(device))\n",
    "        fused_emb_s, _ = fusion(views)\n",
    "        views = mvgat(target_graphs, torch.Tensor(target_norm_poi).to(device))\n",
    "        fused_emb_t, _ = fusion(views)\n",
    "\n",
    "    meta_train_epoch(fused_emb_s, fused_emb_t, net, epoch)\n",
    "    with torch.no_grad():\n",
    "        source_weights = scoring(fused_emb_s, fused_emb_t, th_mask_virtual, th_mask_target)\n",
    "    return source_weights\n",
    "\n",
    "\n",
    "def select_mask(a):\n",
    "    if a == 420:\n",
    "        return dcmask\n",
    "    elif a == 476:\n",
    "        return chimask\n",
    "    elif a == 460:\n",
    "        return nymask\n",
    "    else:\n",
    "        return mask_virtual\n",
    "\n",
    "\n",
    "def train(dur, model, optimizer, total_step, start_step, need_road, train_dataloader, val_dataloader, testdl, type,\n",
    "          weight):\n",
    "    t0 = time.time()\n",
    "    train_mae, val_mae, train_rmse, val_rmse, train_acc = list(), list(), list(), list(), list()\n",
    "    train_correct = 0\n",
    "\n",
    "    model.train()\n",
    "    if type == 'pretrain':\n",
    "        domain_classifier.train()\n",
    "        model.dataset = \"4\"\n",
    "\n",
    "    for i, (feat, label) in enumerate(train_dataloader.get_iterator()):\n",
    "        mask = select_mask(feat.shape[2])\n",
    "        Reverse = False\n",
    "        if i > 0:\n",
    "            if train_acc[-1] > 0.333333:\n",
    "                Reverse = True\n",
    "        p = float(i + start_step) / total_step\n",
    "        constant = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        feat = torch.FloatTensor(feat).to(device)\n",
    "        label = torch.FloatTensor(label).to(device)\n",
    "        if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if args.models not in ['DCRNN', 'STGCN', 'HA']:\n",
    "            if type == 'pretrain':\n",
    "                pred, shared_pems04_feat, shared_pems07_feat, shared_pems08_feat = model(vec_pems04, vec_pems07,\n",
    "                                                                                         vec_pems08, feat, False,\n",
    "                                                                                         need_road, False)\n",
    "            elif type == 'fine-tune':\n",
    "                pred = model(vec_pems04, vec_pems07, vec_pems08, feat, False, need_road)\n",
    "\n",
    "            pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "            label = label.reshape((-1, label.size(2)))\n",
    "\n",
    "            if type == 'pretrain':\n",
    "                pems04_pred = domain_classifier(shared_pems04_feat, constant, Reverse)\n",
    "                pems07_pred = domain_classifier(shared_pems07_feat, constant, Reverse)\n",
    "                pems08_pred = domain_classifier(shared_pems08_feat, constant, Reverse)\n",
    "\n",
    "                pems04_label = 0 * torch.ones(pems04_pred.shape[0]).long().to(device)\n",
    "                pems07_label = 1 * torch.ones(pems07_pred.shape[0]).long().to(device)\n",
    "                pems08_label = 2 * torch.ones(pems08_pred.shape[0]).long().to(device)\n",
    "\n",
    "                pems04_pred_label = pems04_pred.max(1, keepdim=True)[1]\n",
    "                pems04_correct = pems04_pred_label.eq(pems04_label.view_as(pems04_pred_label)).sum()\n",
    "                pems07_pred_label = pems07_pred.max(1, keepdim=True)[1]\n",
    "                pems07_correct = pems07_pred_label.eq(pems07_label.view_as(pems07_pred_label)).sum()\n",
    "                pems08_pred_label = pems08_pred.max(1, keepdim=True)[1]\n",
    "                pems08_correct = pems08_pred_label.eq(pems08_label.view_as(pems08_pred_label)).sum()\n",
    "\n",
    "                mmmm = (th_mask_virtual.reshape((-1)))\n",
    "                pems04_pred = pems04_pred[mmmm.bool(), :]\n",
    "                pems04_label = pems04_label[mmmm.bool()]\n",
    "\n",
    "                pems04_loss = F.nll_loss(pems04_pred, pems04_label)\n",
    "\n",
    "                pems07_loss = domain_criterion(pems07_pred, pems07_label)\n",
    "\n",
    "                mmmm = (th_mask_target.reshape((-1)))\n",
    "                pems08_pred = pems08_pred[mmmm.bool(), :]\n",
    "                pems08_label = pems08_label[mmmm.bool()]\n",
    "                pems08_loss = domain_criterion(pems08_pred, pems08_label)\n",
    "\n",
    "                domain_loss = pems04_loss + pems08_loss\n",
    "\n",
    "        if type == 'pretrain':\n",
    "            train_correct = pems04_correct + pems08_correct\n",
    "\n",
    "        mae_train, rmse_train, mape_train = masked_loss(scaler.inverse_transform(pred), scaler.inverse_transform(label),\n",
    "                                                        maskp=mask, weight=weight)\n",
    "\n",
    "        if type == 'pretrain':\n",
    "            if i == 1:\n",
    "                log(mae_train, domain_loss)\n",
    "            loss = mae_train + args.beta * (args.theta * domain_loss)\n",
    "        elif type == 'fine-tune':\n",
    "            loss = mae_train\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mae.append(mae_train.item())\n",
    "        train_rmse.append(rmse_train.item())\n",
    "\n",
    "        if type == 'pretrain':\n",
    "            train_acc.append(train_correct.item() / 855)\n",
    "        elif type == 'fine-tune':\n",
    "            train_acc.append(0)\n",
    "\n",
    "    if type == 'pretrain':\n",
    "        domain_classifier.eval()\n",
    "    model.eval()\n",
    "\n",
    "    for i, (feat, label) in enumerate(val_dataloader.get_iterator()):\n",
    "        mask = select_mask(feat.shape[2])\n",
    "        feat = torch.FloatTensor(feat).to(device)\n",
    "        label = torch.FloatTensor(label).to(device)\n",
    "        if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            continue\n",
    "        pred = model(vec_pems04, vec_pems07, vec_pems08, feat, True, need_road)\n",
    "        pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "        label = label.reshape((-1, label.size(2)))\n",
    "        mae_val, rmse_val, mape_val = masked_loss(scaler.inverse_transform(pred), scaler.inverse_transform(label),\n",
    "                                                  maskp=mask)\n",
    "        val_mae.append(mae_val.item())\n",
    "        val_rmse.append(rmse_val.item())\n",
    "\n",
    "    test_mae, test_rmse, test_mape = test(testdl, type)\n",
    "    dur.append(time.time() - t0)\n",
    "    return np.mean(train_mae), np.mean(train_rmse), np.mean(val_mae), np.mean(\n",
    "        val_rmse), test_mae, test_rmse, test_mape, np.mean(train_acc)\n",
    "\n",
    "\n",
    "def test(test_dataloader, type):\n",
    "    if type == 'pretrain':\n",
    "        domain_classifier.eval()\n",
    "    model.eval()\n",
    "\n",
    "    test_mape, test_rmse, test_mae = list(), list(), list()\n",
    "\n",
    "    for i, (feat, label) in enumerate(test_dataloader.get_iterator()):\n",
    "        feat = torch.FloatTensor(feat).to(device)\n",
    "        label = torch.FloatTensor(label).to(device)\n",
    "        mask = select_mask(feat.shape[2])\n",
    "        if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            continue\n",
    "\n",
    "        pred = model(vec_pems04, vec_pems07, vec_pems08, feat, True, args.need_road)\n",
    "        pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "        label = label.reshape((-1, label.size(2)))\n",
    "\n",
    "        mae_test, rmse_test, mape_test = masked_loss(scaler.inverse_transform(pred), scaler.inverse_transform(label),\n",
    "                                                     maskp=mask)\n",
    "\n",
    "        test_mae.append(mae_test.item())\n",
    "        test_rmse.append(rmse_test.item())\n",
    "        test_mape.append(mape_test.item())\n",
    "\n",
    "    test_rmse = np.mean(test_rmse)\n",
    "    test_mae = np.mean(test_mae)\n",
    "    test_mape = np.mean(test_mape)\n",
    "\n",
    "    return test_mae, test_rmse, test_mape\n",
    "\n",
    "\n",
    "def model_train(args, model, optimizer, train_dataloader, val_dataloader, test_dataloader, type):\n",
    "    dur = []\n",
    "    epoch = 1\n",
    "    best = 999999999999999\n",
    "    acc = list()\n",
    "\n",
    "    step_per_epoch = train_dataloader.get_num_batch()\n",
    "    total_step = 200 * step_per_epoch\n",
    "\n",
    "    while epoch <= args.epoch:\n",
    "        if type == 'pretrain' and args.need_weight == 1:\n",
    "            source_weights = get_weight(model, type, epoch)\n",
    "            if epoch == 1:\n",
    "                source_weights_ma = torch.ones_like(source_weights, device=device, requires_grad=False)\n",
    "            source_weights_ma = cross_ma_param * source_weights_ma + (1 - cross_ma_param) * source_weights\n",
    "            # shows = np.zeros((virtual_city.shape[1], virtual_city.shape[2]))\n",
    "            # count = 0\n",
    "            # for p in range(virtual_city.shape[1]):\n",
    "            #     for q in range(virtual_city.shape[2]):\n",
    "            #         if mask_virtual[p][q]:\n",
    "            #             shows[p][q] = source_weights_ma[count]\n",
    "            #             count = count + 1\n",
    "            # heatmap = seaborn.heatmap(shows)\n",
    "            # fig = heatmap.get_figure()\n",
    "            # fig.savefig(local_path_generate(\"\", \"{}\".format(str(epoch)), \".png\" ))\n",
    "            # fig.show()\n",
    "\n",
    "            log(source_weights_ma.mean())\n",
    "        else:\n",
    "            source_weights_ma = None\n",
    "        start_step = epoch * step_per_epoch\n",
    "        if type == 'fine-tune' and epoch > 1000:\n",
    "            args.val = True\n",
    "        mae_train, rmse_train, mae_val, rmse_val, mae_test, rmse_test, mape_test, train_acc = train(dur, model,\n",
    "                                                                                                    optimizer,\n",
    "                                                                                                    total_step,\n",
    "                                                                                                    start_step,\n",
    "                                                                                                    args.need_road,\n",
    "                                                                                                    train_dataloader,\n",
    "                                                                                                    val_dataloader,\n",
    "                                                                                                    test_dataloader,\n",
    "                                                                                                    type,\n",
    "                                                                                                    source_weights_ma)\n",
    "        log(f'Epoch {epoch} | acc_train: {train_acc: .4f} | mae_train: {mae_train: .4f} | rmse_train: {rmse_train: .4f} | mae_val: {mae_val: .4f} | rmse_val: {rmse_val: .4f} | mae_test: {mae_test: .4f} | rmse_test: {rmse_test: .4f} | mape_test: {mape_test: .4f} | Time(s) {dur[-1]: .4f}')\n",
    "        epoch += 1\n",
    "        acc.append(train_acc)\n",
    "        if args.need_weight == 0 or type == \"fine-tune\":\n",
    "            if mae_val <= best:\n",
    "                if type == 'fine-tune' and mae_val > 0.001:\n",
    "                    best = mae_val\n",
    "                    state = dict([('model', copy.deepcopy(model.state_dict())),\n",
    "                                  ('optim', copy.deepcopy(optimizer.state_dict())),\n",
    "                                  ('domain_classifier', copy.deepcopy(domain_classifier.state_dict()))])\n",
    "                    cnt = 0\n",
    "                elif type == 'pretrain':\n",
    "                    best = mae_val\n",
    "                    state = dict([('model', copy.deepcopy(model.state_dict())),\n",
    "                                  ('optim', copy.deepcopy(optimizer.state_dict())),\n",
    "                                  ('domain_classifier', copy.deepcopy(domain_classifier.state_dict()))])\n",
    "                    cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "            if cnt == args.patience or epoch > args.epoch:\n",
    "                log(f'Stop!!')\n",
    "                log(f'Avg acc: {np.mean(acc)}')\n",
    "                break\n",
    "        else:\n",
    "            # fast_weights, bn_vars = get_weights_bn_vars(model)\n",
    "            # for i in range(args.simu_fine_epoch):\n",
    "            #     for i, (feat, label) in enumerate(ttl.get_iterator()):\n",
    "            #         mask = select_mask(feat.shape[2])\n",
    "            #         feat = torch.FloatTensor(feat).to(device)\n",
    "            #         label = torch.FloatTensor(label).to(device)\n",
    "            #         if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            #             continue\n",
    "            #         pred = model.functional_forward(vec_pems04,\n",
    "            #                                         vec_pems07,\n",
    "            #                                         vec_pems08,\n",
    "            #                                         feat, True,\n",
    "            #                                         fast_weights,\n",
    "            #                                         bn_vars,\n",
    "            #                                         bn_training=True,\n",
    "            #                                         data_set=\"8\")\n",
    "            #         pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "            #         label = label.reshape((-1, label.size(2)))\n",
    "            #         mae_val, rmse_val, mape_val = masked_loss(scaler.inverse_transform(pred),\n",
    "            #                                                   scaler.inverse_transform(label), maskp=mask)\n",
    "            #         a = [(i, torch.autograd.grad(mae_val, fast_weights[i], create_graph=True, allow_unused=True)) for i in\n",
    "            #              fast_weights.keys()]\n",
    "            #         grads = {}\n",
    "            #         used_fast_weight = OrderedDict()\n",
    "            #         for i in a:\n",
    "            #             if i[1][0] is not None:\n",
    "            #                 grads[i[0]] = i[1][0]\n",
    "            #                 used_fast_weight[i[0]] = fast_weights[i[0]]\n",
    "            #\n",
    "            #         for name, grad in zip(grads.keys(), grads.values()):\n",
    "            #             fast_weights[name] = fast_weights[name] - args.innerlr * grad\n",
    "            # val_mae = []\n",
    "            # for i, (feat, label) in enumerate(tvl.get_iterator()):\n",
    "            #     mask = select_mask(feat.shape[2])\n",
    "            #     feat = torch.FloatTensor(feat).to(device)\n",
    "            #     label = torch.FloatTensor(label).to(device)\n",
    "            #     if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            #         continue\n",
    "            #     pred = model.functional_forward(vec_pems04,\n",
    "            #                                     vec_pems07,\n",
    "            #                                     vec_pems08,\n",
    "            #                                     feat, True,\n",
    "            #                                     fast_weights,\n",
    "            #                                     bn_vars,\n",
    "            #                                     bn_training=True,\n",
    "            #                                     data_set=\"8\")\n",
    "            #     pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "            #     label = label.reshape((-1, label.size(2)))\n",
    "            #     mae_val, rmse_val, mape_val = masked_loss(scaler.inverse_transform(pred),\n",
    "            #                                               scaler.inverse_transform(label), maskp=mask)\n",
    "            #     val_mae.append(mae_val.item())\n",
    "            # log(np.mean(val_mae))\n",
    "            if source_weights_ma.mean() <= best and source_weights_ma.mean() > 0.005:\n",
    "                best = source_weights_ma.mean().cpu().numpy().item()\n",
    "                state = dict([('model', copy.deepcopy(model.state_dict())),\n",
    "                              ('optim', copy.deepcopy(optimizer.state_dict())),\n",
    "                              ('domain_classifier', copy.deepcopy(domain_classifier.state_dict()))])\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "            if cnt == args.patience or epoch > args.epoch:\n",
    "                log(f'Stop!!')\n",
    "                log(f'Avg acc: {np.mean(acc)}')\n",
    "                break\n",
    "    log(\"Optimization Finished!\")\n",
    "    return state\n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "if cur_dir[-2:] == 'sh':\n",
    "    cur_dir = cur_dir[:-2]\n",
    "\n",
    "pems04_emb_path = os.path.join('{}'.format(cur_dir), 'embeddings', 'node2vec', 'pems04',\n",
    "                               '{}_vecdim.pkl'.format(args.vec_dim))\n",
    "pems07_emb_path = os.path.join('{}'.format(cur_dir), 'embeddings', 'node2vec', 'pems07',\n",
    "                               '{}_vecdim.pkl'.format(args.vec_dim))\n",
    "pems08_emb_path = os.path.join('{}'.format(cur_dir), 'embeddings', 'node2vec', 'pems08',\n",
    "                               '{}_vecdim.pkl'.format(args.vec_dim))\n",
    "v_p = os.path.join('{}'.format(cur_dir), 'embeddings', 'node2vec', 'vc',\n",
    "                   '{}{}{}{}{}{}{}_vecdim.pkl'.format(args.vec_dim, args.dataname, args.datatype,\n",
    "                                                      str(args.s1_rate).replace(\".\", \"\"),\n",
    "                                                      str(args.s2_rate).replace(\".\", \"\"),\n",
    "                                                      str(args.s3_rate).replace(\".\", \"\"),\n",
    "                                                      get_timestamp(split=\"-\")))\n",
    "\n",
    "for i in [pems04_emb_path, pems07_emb_path, pems08_emb_path, v_p]:\n",
    "    a = i.split(os.path.sep)\n",
    "    b = []\n",
    "    for i in a:\n",
    "        if \"pkl\" in i:\n",
    "            continue\n",
    "        else:\n",
    "            b.append(i)\n",
    "    local_path_generate(folder_name=os.path.sep.join(b), create_folder_only=True)\n",
    "\n",
    "if os.path.exists(pems04_emb_path):\n",
    "    log(f'Loading pems04 embedding...')\n",
    "    vec_pems04 = torch.load(pems04_emb_path, map_location='cpu')\n",
    "    vec_pems04 = vec_pems04.to(device)\n",
    "else:\n",
    "    log(f'Generating pems04 embedding...')\n",
    "    args.dataset = '4'\n",
    "    vec_pems04, _ = generate_vector(adj_pems04.cpu().numpy(), args)\n",
    "    vec_pems04 = vec_pems04.to(device)\n",
    "    log(f'Saving pems04 embedding...')\n",
    "    torch.save(vec_pems04.cpu(), pems04_emb_path)\n",
    "\n",
    "if os.path.exists(pems07_emb_path):\n",
    "    log(f'Loading pems07 embedding...')\n",
    "    vec_pems07 = torch.load(pems07_emb_path, map_location='cpu')\n",
    "    vec_pems07 = vec_pems07.to(device)\n",
    "else:\n",
    "    log(f'Generating pems07 embedding...')\n",
    "    args.dataset = '7'\n",
    "    vec_pems07, _ = generate_vector(adj_pems07.cpu().numpy(), args)\n",
    "    vec_pems07 = vec_pems07.to(device)\n",
    "    log(f'Saving pems07 embedding...')\n",
    "    torch.save(vec_pems07.cpu(), pems07_emb_path)\n",
    "\n",
    "if os.path.exists(pems08_emb_path):\n",
    "    log(f'Loading pems08 embedding...')\n",
    "    vec_pems08 = torch.load(pems08_emb_path, map_location='cpu')\n",
    "    vec_pems08 = vec_pems08.to(device)\n",
    "else:\n",
    "    log(f'Generating pems08 embedding...')\n",
    "    args.dataset = '8'\n",
    "    vec_pems08, _ = generate_vector(adj_pems08.cpu().numpy(), args)\n",
    "    vec_pems08 = vec_pems08.to(device)\n",
    "    log(f'Saving pems08 embedding...')\n",
    "    torch.save(vec_pems08.cpu(), pems08_emb_path)\n",
    "\n",
    "if os.path.exists(v_p):\n",
    "    log(f'Loading v embedding...')\n",
    "    vec_virtual = torch.load(v_p, map_location='cpu')\n",
    "    vec_virtual = vec_virtual.to(device)\n",
    "else:\n",
    "    log(f'Generating virtual embedding...')\n",
    "    args.dataset = '8'\n",
    "    vec_virtual, _ = generate_vector(virtual_road, args)\n",
    "    vec_virtual = vec_virtual.to(device)\n",
    "    log(f'Saving virtual embedding...')\n",
    "    torch.save(vec_virtual.cpu(), v_p)\n",
    "\n",
    "log(\n",
    "    f'Successfully load embeddings, 4: {vec_pems04.shape}, 7: {vec_pems07.shape}, 8: {vec_pems08.shape}, vec_virtual:{vec_virtual.shape}')\n",
    "\n",
    "domain_criterion = torch.nn.NLLLoss()\n",
    "domain_classifier = Domain_classifier_DG(num_class=3, encode_dim=args.enc_dim)\n",
    "\n",
    "domain_classifier = domain_classifier.to(device)\n",
    "state = g = None, None\n",
    "\n",
    "batch_seen = 0\n",
    "cur_dir = os.getcwd()\n",
    "if cur_dir[-2:] == 'sh':\n",
    "    cur_dir = cur_dir[:-2]\n",
    "assert args.models in [\"DASTNet\"]\n",
    "\n",
    "bak_epoch = args.epoch\n",
    "bak_val = args.val\n",
    "bak_test = args.test\n",
    "type = 'pretrain'\n",
    "pretrain_model_path = os.path.join('{}'.format(cur_dir), 'pretrained', 'transfer_models',\n",
    "                                   '{}'.format(args.dataset), '{}_prelen'.format(args.pre_len),\n",
    "                                   'flow_model4_{}_epoch_{}_{}_{}_{}_{}_{}{}{}{}.pkl'.format(\n",
    "                                       args.models, args.epoch, args.dataname, args.datatype,\n",
    "                                       str(args.s1_rate).replace(\".\", \"\"),\n",
    "                                       str(args.s2_rate).replace(\".\", \"\"),\n",
    "                                       str(args.s3_rate).replace(\".\", \"\"),\n",
    "                                       str(args.learning_rate),\n",
    "                                       str(args.batch_size),\n",
    "                                       str(args.split_ratio)\n",
    "                                   )\n",
    "                                   )\n",
    "\n",
    "a = pretrain_model_path.split(os.path.sep)\n",
    "b = []\n",
    "for i in a:\n",
    "    if \"pkl\" not in i:\n",
    "        b.append(i)\n",
    "local_path_generate(os.path.sep.join(b), create_folder_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_pems04 = vec_virtual\n",
    "adj_pems04 = adj_virtual\n",
    "args.dataset = \"8\"\n",
    "databak = args.dataset\n",
    "args.dataset = \"8\"\n",
    "ttl, tvl, testtl = get_target_loader(args)\n",
    "args.dataset = databak\n",
    "if os.path.exists(pretrain_model_path):\n",
    "    log(f'Loading pretrained model at {pretrain_model_path}')\n",
    "    state = torch.load(pretrain_model_path, map_location='cpu')\n",
    "else:\n",
    "    log(f'No existing pretrained model at {pretrain_model_path}')\n",
    "    args.val = args.test = False\n",
    "    datasets = [\"4\"]\n",
    "    dataset_bak = args.dataset\n",
    "    labelrate_bak = args.labelrate\n",
    "    args.labelrate = 100\n",
    "    dataset_count = 0\n",
    "\n",
    "    for dataset in [item for item in datasets if item not in [dataset_bak]]:\n",
    "        dataset_count = dataset_count + 1\n",
    "\n",
    "        log(\n",
    "            f'\\n\\n****************************************************************************************************************')\n",
    "        log(f'dataset: {dataset}, model: {args.models}, pre_len: {args.pre_len}, labelrate: {args.labelrate}')\n",
    "        log(\n",
    "            f'****************************************************************************************************************\\n\\n')\n",
    "\n",
    "        if dataset == '4':\n",
    "            g = vec_pems04\n",
    "        elif dataset == '7':\n",
    "            g = vec_pems07\n",
    "        elif dataset == '8':\n",
    "            g = vec_pems08\n",
    "\n",
    "        args.dataset = dataset\n",
    "\n",
    "\n",
    "        def load_graphdata_channel3(args, feat_dir, time, scaler=None, visualize=False, cut=False):\n",
    "            data = virtual_city\n",
    "            data = data.reshape((data.shape[0], data.shape[1] * data.shape[2]))\n",
    "            log(data.shape)\n",
    "            if time:\n",
    "                num_data, num_sensor = data.shape\n",
    "                data = np.expand_dims(data, axis=-1)\n",
    "                data = data.tolist()\n",
    "\n",
    "                for i in range(num_data):\n",
    "                    time = (i % 288) / 288\n",
    "                    for j in range(num_sensor):\n",
    "                        data[i][j].append(time)\n",
    "\n",
    "                data = np.array(data)\n",
    "\n",
    "            max_val = np.max(data)\n",
    "            time_len = data.shape[0]\n",
    "            seq_len = args.seq_len\n",
    "            pre_len = args.pre_len\n",
    "            split_ratio = args.split_ratio\n",
    "            train_size = int(time_len * split_ratio)\n",
    "            val_size = int(time_len * (1 - split_ratio) / 3)\n",
    "            train_data = data[:train_size]\n",
    "            val_data = data[train_size:train_size + val_size]\n",
    "            test_data = data[train_size + val_size:time_len]\n",
    "            log(data.shape)\n",
    "            if args.labelrate != 100:\n",
    "                import random\n",
    "                new_train_size = int(train_size * args.labelrate / 100)\n",
    "                start = random.randint(0, train_size - new_train_size - 1)\n",
    "                train_data = train_data[start:start + new_train_size]\n",
    "\n",
    "            train_X, train_Y, val_X, val_Y, test_X, test_Y = list(), list(), list(), list(), list(), list()\n",
    "\n",
    "            for i in range(len(train_data) - seq_len - pre_len):\n",
    "                train_X.append(np.array(train_data[i: i + seq_len]))\n",
    "                train_Y.append(np.array(train_data[i + seq_len: i + seq_len + pre_len]))\n",
    "            for i in range(len(val_data) - seq_len - pre_len):\n",
    "                val_X.append(np.array(val_data[i: i + seq_len]))\n",
    "                val_Y.append(np.array(val_data[i + seq_len: i + seq_len + pre_len]))\n",
    "            for i in range(len(test_data) - seq_len - pre_len):\n",
    "                test_X.append(np.array(test_data[i: i + seq_len]))\n",
    "                test_Y.append(np.array(test_data[i + seq_len: i + seq_len + pre_len]))\n",
    "\n",
    "            if visualize:\n",
    "                test_X = test_X[-288:]\n",
    "                test_Y = test_Y[-288:]\n",
    "\n",
    "            if args.labelrate != 0:\n",
    "                train_X = np.array(train_X)\n",
    "                train_Y = np.array(train_Y)\n",
    "            val_X = np.array(val_X)\n",
    "            val_Y = np.array(val_Y)\n",
    "            test_X = np.array(test_X)\n",
    "            test_Y = np.array(test_Y)\n",
    "\n",
    "            if args.labelrate != 0:\n",
    "                max_xtrain = np.max(train_X)\n",
    "                max_ytrain = np.max(train_Y)\n",
    "            max_xval = np.max(val_X)\n",
    "            max_yval = np.max(val_Y)\n",
    "            max_xtest = np.max(test_X)\n",
    "            max_ytest = np.max(test_Y)\n",
    "\n",
    "            if args.labelrate != 0:\n",
    "                min_xtrain = np.min(train_X)\n",
    "                min_ytrain = np.min(train_Y)\n",
    "            min_xval = np.min(val_X)\n",
    "            min_yval = np.min(val_Y)\n",
    "            min_xtest = np.min(test_X)\n",
    "            min_ytest = np.min(test_Y)\n",
    "\n",
    "            if args.labelrate != 0:\n",
    "                max_speed = max(max_xtrain, max_ytrain, max_xval, max_yval, max_xtest, max_ytest)\n",
    "                min_speed = min(min_xtrain, min_ytrain, min_xval, min_yval, min_xtest, min_ytest)\n",
    "\n",
    "                # scaler = StandardScaler(mean=train_X[..., 0].mean(), std=train_X[..., 0].std())\n",
    "                scaler = StandardScaler(mean=train_X.mean(), std=train_X.std())\n",
    "\n",
    "                train_X = scaler.transform(train_X)\n",
    "                train_Y = scaler.transform(train_Y)\n",
    "            else:\n",
    "                max_speed = max(max_xval, max_yval, max_xtest, max_ytest)\n",
    "                min_speed = min(min_xval, min_yval, min_xtest, min_ytest)\n",
    "\n",
    "            val_X = scaler.transform(val_X)\n",
    "            val_Y = scaler.transform(val_Y)\n",
    "            test_X = scaler.transform(test_X)\n",
    "            test_Y = scaler.transform(test_Y)\n",
    "\n",
    "            if args.labelrate != 0:\n",
    "                max_xtrain = np.max(train_X)\n",
    "                max_ytrain = np.max(train_Y)\n",
    "            max_xval = np.max(val_X)\n",
    "            max_yval = np.max(val_Y)\n",
    "            max_xtest = np.max(test_X)\n",
    "            max_ytest = np.max(test_Y)\n",
    "\n",
    "            if args.labelrate != 0:\n",
    "                min_xtrain = np.min(train_X)\n",
    "                min_ytrain = np.min(train_Y)\n",
    "            min_xval = np.min(val_X)\n",
    "            min_yval = np.min(val_Y)\n",
    "            min_xtest = np.min(test_X)\n",
    "            min_ytest = np.min(test_Y)\n",
    "\n",
    "            if args.labelrate != 0:\n",
    "                max_speed = max(max_xtrain, max_ytrain, max_xval, max_yval, max_xtest, max_ytest)\n",
    "                min_speed = min(min_xtrain, min_ytrain, min_xval, min_yval, min_xtest, min_ytest)\n",
    "\n",
    "            else:\n",
    "                max_speed = max(max_xval, max_yval, max_xtest, max_ytest)\n",
    "                min_speed = min(min_xval, min_yval, min_xtest, min_ytest)\n",
    "            if cut:\n",
    "                train_X = train_X[-args.data_amount * 24:, :, :]\n",
    "                train_Y = train_Y[-args.data_amount * 24:, :, :]\n",
    "            return train_X, train_Y, val_X, val_Y, test_X, test_Y, max_val, scaler\n",
    "\n",
    "\n",
    "        train_dataloader, val_dataloader, test_dataloader, adj, max_speed, scaler = load_data(args)\n",
    "        train_X, train_Y, val_X, val_Y, test_X, test_Y, max_speed, scaler = load_graphdata_channel3(args, \"\", False,\n",
    "                                                                                                    scaler,\n",
    "                                                                                                    visualize=False)\n",
    "        log([i.shape for i in [train_X, train_Y, val_X, val_Y, test_X, test_Y]])\n",
    "        train_dataloader = MyDataLoader(torch.FloatTensor(train_X), torch.FloatTensor(train_Y),\n",
    "                                        batch_size=args.batch_size)\n",
    "        val_dataloader = MyDataLoader(torch.FloatTensor(val_X), torch.FloatTensor(val_Y), batch_size=args.batch_size)\n",
    "        test_dataloader = MyDataLoader(torch.FloatTensor(test_X), torch.FloatTensor(test_Y), batch_size=args.batch_size)\n",
    "        adj = 0\n",
    "        model = DASTNet(input_dim=args.vec_dim, hidden_dim=args.hidden_dim, encode_dim=args.enc_dim,\n",
    "                        device=device, batch_size=args.batch_size, etype=args.etype, pre_len=args.pre_len,\n",
    "                        dataset=args.dataset, ft_dataset=dataset_bak,\n",
    "                        adj_pems04=adj_pems04, adj_pems07=adj_pems07, adj_pems08=adj_pems08).to(device)\n",
    "        optimizer = optim.SGD([{'params': model.parameters()},\n",
    "                               {'params': domain_classifier.parameters()}], lr=args.learning_rate, momentum=0.8)\n",
    "\n",
    "        if dataset_count != 1:\n",
    "            model.load_state_dict(state['model'])\n",
    "            optimizer.load_state_dict(state['optim'])\n",
    "\n",
    "        state = model_train(args, model, optimizer, train_dataloader, val_dataloader, test_dataloader, type)\n",
    "\n",
    "    log(f'Saving model to {pretrain_model_path} ...')\n",
    "    torch.save(state, pretrain_model_path)\n",
    "    args.dataset = dataset_bak\n",
    "    args.labelrate = labelrate_bak\n",
    "    args.val = bak_val\n",
    "    args.test = bak_test\n",
    "\n",
    "type = 'fine-tune'\n",
    "args.epoch = args.fine_epoch\n",
    "args.dataset = \"8\"\n",
    "log(f'\\n\\n*******************************************************************************************')\n",
    "log(\n",
    "    f'dataset: {args.dataset}, model: {args.models}, pre_len: {args.pre_len}, labelrate: {args.labelrate}, seed: {args.division_seed}')\n",
    "log(f'*******************************************************************************************\\n\\n')\n",
    "\n",
    "if args.dataset == '4':\n",
    "    g = vec_pems04\n",
    "elif args.dataset == '7':\n",
    "    g = vec_pems07\n",
    "elif args.dataset == '8':\n",
    "    g = vec_pems08\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, adj, max_speed, scaler = load_data(args, cut=True)\n",
    "model = DASTNet(input_dim=args.vec_dim, hidden_dim=args.hidden_dim, encode_dim=args.enc_dim,\n",
    "                device=device, batch_size=args.batch_size, etype=args.etype, pre_len=args.pre_len,\n",
    "                dataset=args.dataset, ft_dataset=args.dataset,\n",
    "                adj_pems04=adj_pems04, adj_pems07=adj_pems07, adj_pems08=adj_pems08).to(device)\n",
    "optimizer = optim.SGD([{'params': model.parameters()},\n",
    "                       {'params': domain_classifier.parameters()}], lr=args.learning_rate, momentum=0.8)\n",
    "model.load_state_dict(state['model'])\n",
    "optimizer.load_state_dict(state['optim'])\n",
    "\n",
    "if args.labelrate != 0:\n",
    "    test_state = model_train(args, model, optimizer, train_dataloader, val_dataloader, test_dataloader, type)\n",
    "    model.load_state_dict(test_state['model'])\n",
    "    optimizer.load_state_dict(test_state['optim'])\n",
    "\n",
    "test_mae, test_rmse, test_mape = test(test_dataloader, type)\n",
    "log(f'mae: {test_mae: .4f}, rmse: {test_rmse: .4f}, mape: {test_mape * 100: .4f}\\n\\n')\n",
    "if args.c != \"default\":\n",
    "    if args.need_remark == 1:\n",
    "        record.update(record_id, get_timestamp(),\n",
    "                      \"%.4f,%.4f,%.4f\" %\n",
    "                      (test_rmse, test_mae, test_mape * 100),\n",
    "                      remark=\"{}C {} {} {} {}\".format(\"2\" if args.need_third == 0 else \"3\", str(args.data_amount),\n",
    "                                                      args.dataname, args.datatype, args.machine_code))\n",
    "    else:\n",
    "        record.update(record_id, get_timestamp(),\n",
    "                      \"%.4f,%.4f, %.4f\" %\n",
    "                      (test_rmse, test_mae, test_mape * 100),\n",
    "                      remark=\"{}\".format(args.machine_code))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
