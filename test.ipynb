{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(alpha=0.1, batch_size=64, beta=0.2, c='default', data_amount=3, dataname='Taxi', dataset='4', datatype='pickup', device=0, division_seed=0, enc_dim=64, epoch=10, etype='gin', fine_epoch=80, hidden_dim=64, labelrate=23, learning_rate=0.0001, machine_code='my-1060', model='DASTNet', need_remark=0, need_road=True, need_third=0, normalize=True, num_walks=200, p=1, patience=200, pre_len=3, q=1, seed=0, seq_len=12, split_ratio=0.7, test=False, theta=1, train=False, val=False, vec_dim=64, walk_length=8)\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\install\\envs\\CTRS\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1048576 into shape (1048576,1048576)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [2], line 300\u001B[0m\n\u001B[0;32m    294\u001B[0m             dc \u001B[38;5;241m=\u001B[39m t\n\u001B[0;32m    296\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(ny)\u001B[38;5;241m.\u001B[39mto(device), torch\u001B[38;5;241m.\u001B[39mtensor(chi)\u001B[38;5;241m.\u001B[39mto(device), torch\u001B[38;5;241m.\u001B[39mtensor(dc)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m--> 300\u001B[0m adj_pems04, adj_pems07, adj_pems08 \u001B[38;5;241m=\u001B[39m load_all_adj(device)\n\u001B[0;32m    301\u001B[0m vec_pems04 \u001B[38;5;241m=\u001B[39m vec_pems07 \u001B[38;5;241m=\u001B[39m vec_pems08 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    302\u001B[0m dc \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data/DC/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mDC_\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(args\u001B[38;5;241m.\u001B[39mdataname, args\u001B[38;5;241m.\u001B[39mdatatype))\n",
      "Cell \u001B[1;32mIn [2], line 279\u001B[0m, in \u001B[0;36mload_all_adj\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    277\u001B[0m t \u001B[38;5;241m=\u001B[39m dirs\u001B[38;5;241m.\u001B[39mformat(i, i)\n\u001B[0;32m    278\u001B[0m t \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(t)\n\u001B[1;32m--> 279\u001B[0m t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    280\u001B[0m t \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(t \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, t)\n\u001B[0;32m    281\u001B[0m t \u001B[38;5;241m=\u001B[39m add_self_loop(t)\n",
      "\u001B[1;31mValueError\u001B[0m: cannot reshape array of size 1048576 into shape (1048576,1048576)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2023/3/31 16:39\n",
    "# @Author  : 银尘\n",
    "# @FileName: main3.py\n",
    "# @Software: PyCharm\n",
    "# @Email   : liwudi@liwudi.fun\n",
    "# @Info    : 单城市\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from utils.funcs import load_data, load_all_adj\n",
    "from utils.funcs import masked_loss\n",
    "from utils.vec import generate_vector\n",
    "from model import DASTNet, Domain_classifier_DG\n",
    "from PaperCrawlerUtil.common_util import *\n",
    "from PaperCrawlerUtil.research_util import *\n",
    "import ast\n",
    "\n",
    "basic_config(logs_style=LOG_STYLE_ALL)\n",
    "\n",
    "\n",
    "def arg_parse(parser):\n",
    "    parser.add_argument('--dataset', type=str, default='4', help='dataset')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='seed')\n",
    "    parser.add_argument('--division_seed', type=int, default=0, help='division_seed')\n",
    "    parser.add_argument('--model', type=str, default='DASTNet', help='model')\n",
    "    parser.add_argument('--labelrate', type=float, default=23, help='percent')\n",
    "    parser.add_argument('--patience', type=int, default=200, help='patience')\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=64)\n",
    "    parser.add_argument(\"--vec_dim\", type=int, default=64)\n",
    "    parser.add_argument(\"--enc_dim\", type=int, default=64)\n",
    "    parser.add_argument(\"--walk_length\", \"--wl\", type=int, default=8)\n",
    "    parser.add_argument(\"--num_walks\", type=int, default=200)\n",
    "    parser.add_argument(\"--theta\", type=float, default=1)\n",
    "    parser.add_argument(\"--p\", type=float, default=1)\n",
    "    parser.add_argument(\"--q\", type=float, default=1)\n",
    "    parser.add_argument(\"--learning_rate\", \"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "    parser.add_argument('--device', type=int, default=0, help='CUDA Device')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=12)\n",
    "    parser.add_argument(\"--pre_len\", type=int, default=3)\n",
    "    parser.add_argument(\"--split_ratio\", type=float, default=0.7)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--beta\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--normalize\", type=bool, default=True)\n",
    "    parser.add_argument('--val', action='store_true', default=False, help='eval')\n",
    "    parser.add_argument('--test', action='store_true', default=False, help='test')\n",
    "    parser.add_argument('--train', action='store_true', default=False, help='train')\n",
    "    parser.add_argument('--etype', type=str, default=\"gin\", choices=[\"gin\"], help='feature type')\n",
    "    parser.add_argument('--dataname', type=str, default='Taxi', help='Within [Bike, Taxi]')\n",
    "    parser.add_argument('--datatype', type=str, default='pickup', help='Within [pickup, dropoff]')\n",
    "    parser.add_argument('--data_amount', type=int, default=3, help='0: full data, 30/7/3 correspond to days of data')\n",
    "    parser.add_argument('--need_third', type=int, default=0)\n",
    "    parser.add_argument(\"--c\", type=str, default=\"default\", help=\"research record\")\n",
    "    parser.add_argument(\"--machine_code\", type=str, default=\"my-1060\", help=\"code of machine\")\n",
    "    parser.add_argument(\"--need_remark\", type=int, default=0)\n",
    "    parser.add_argument(\"--fine_epoch\", type=int, default=80)\n",
    "    parser.add_argument(\"--need_road\", action='store_false', default=True, help='test')\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "def select_mask(a):\n",
    "    if a == 420:\n",
    "        return dcmask\n",
    "    elif a == 476:\n",
    "        return chimask\n",
    "    elif a == 460:\n",
    "        return nymask\n",
    "\n",
    "\n",
    "def train(dur, model, optimizer, total_step, start_step):\n",
    "    t0 = time.time()\n",
    "    train_mae, val_mae, train_rmse, val_rmse, train_acc = list(), list(), list(), list(), list()\n",
    "    train_correct = 0\n",
    "\n",
    "    model.train()\n",
    "    if type == 'pretrain':\n",
    "        domain_classifier.train()\n",
    "\n",
    "    for i, (feat, label) in enumerate(train_dataloader.get_iterator()):\n",
    "        mask = select_mask(feat.shape[2])\n",
    "        Reverse = False\n",
    "        if i > 0:\n",
    "            if train_acc[-1] > 0.333333:\n",
    "                Reverse = True\n",
    "        p = float(i + start_step) / total_step\n",
    "        constant = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        feat = torch.FloatTensor(feat).to(device)\n",
    "        label = torch.FloatTensor(label).to(device)\n",
    "        if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if args.model not in ['DCRNN', 'STGCN', 'HA']:\n",
    "            if type == 'pretrain':\n",
    "                pred, shared_pems04_feat, shared_pems07_feat, shared_pems08_feat = model(vec_pems04, vec_pems07,\n",
    "                                                                                         vec_pems08, feat, False,\n",
    "                                                                                         need_road=args.need_road)\n",
    "            elif type == 'fine-tune':\n",
    "                pred = model(vec_pems04, vec_pems07, vec_pems08, feat, False)\n",
    "\n",
    "            pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "            label = label.reshape((-1, label.size(2)))\n",
    "\n",
    "            if type == 'pretrain' and args.need_road == True:\n",
    "                pems04_pred = domain_classifier(shared_pems04_feat, constant, Reverse)\n",
    "                pems07_pred = domain_classifier(shared_pems07_feat, constant, Reverse)\n",
    "                pems08_pred = domain_classifier(shared_pems08_feat, constant, Reverse)\n",
    "\n",
    "                pems04_label = 0 * torch.ones(pems04_pred.shape[0]).long().to(device)\n",
    "                pems07_label = 1 * torch.ones(pems07_pred.shape[0]).long().to(device)\n",
    "                pems08_label = 2 * torch.ones(pems08_pred.shape[0]).long().to(device)\n",
    "\n",
    "                pems04_pred_label = pems04_pred.max(1, keepdim=True)[1]\n",
    "                pems04_correct = pems04_pred_label.eq(pems04_label.view_as(pems04_pred_label)).sum()\n",
    "                pems07_pred_label = pems07_pred.max(1, keepdim=True)[1]\n",
    "                pems07_correct = pems07_pred_label.eq(pems07_label.view_as(pems07_pred_label)).sum()\n",
    "                pems08_pred_label = pems08_pred.max(1, keepdim=True)[1]\n",
    "                pems08_correct = pems08_pred_label.eq(pems08_label.view_as(pems08_pred_label)).sum()\n",
    "\n",
    "                pems04_loss = domain_criterion(pems04_pred, pems04_label)\n",
    "                pems07_loss = domain_criterion(pems07_pred, pems07_label)\n",
    "                pems08_loss = domain_criterion(pems08_pred, pems08_label)\n",
    "\n",
    "                domain_loss = pems04_loss + pems08_loss + pems07_loss\n",
    "\n",
    "        if type == 'pretrain' and args.need_road:\n",
    "            train_correct = pems04_correct + pems08_correct\n",
    "\n",
    "        mae_train, rmse_train, mape_train = masked_loss(scaler.inverse_transform(pred), scaler.inverse_transform(label),\n",
    "                                                        maskp=mask)\n",
    "\n",
    "        if type == 'pretrain' and args.need_road:\n",
    "            loss = mae_train + args.beta * (args.theta * domain_loss)\n",
    "        elif type == 'pretrain':\n",
    "            loss = mae_train\n",
    "        elif type == 'fine-tune':\n",
    "            loss = mae_train\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mae.append(mae_train.item())\n",
    "        train_rmse.append(rmse_train.item())\n",
    "\n",
    "        if type == 'pretrain' and args.need_road:\n",
    "            train_acc.append(train_correct.item() / 855)\n",
    "        elif type == 'pretrain':\n",
    "            train_acc.append(0)\n",
    "        elif type == 'fine-tune':\n",
    "            train_acc.append(0)\n",
    "\n",
    "    if type == 'pretrain':\n",
    "        domain_classifier.eval()\n",
    "    model.eval()\n",
    "\n",
    "    for i, (feat, label) in enumerate(val_dataloader.get_iterator()):\n",
    "        mask = select_mask(feat.shape[2])\n",
    "        feat = torch.FloatTensor(feat).to(device)\n",
    "        label = torch.FloatTensor(label).to(device)\n",
    "        if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            continue\n",
    "        pred = model(vec_pems04, vec_pems07, vec_pems08, feat, True)\n",
    "        pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "        label = label.reshape((-1, label.size(2)))\n",
    "        mae_val, rmse_val, mape_val = masked_loss(scaler.inverse_transform(pred), scaler.inverse_transform(label),\n",
    "                                                  maskp=mask)\n",
    "        val_mae.append(mae_val.item())\n",
    "        val_rmse.append(rmse_val.item())\n",
    "\n",
    "    test_mae, test_rmse, test_mape = test()\n",
    "    dur.append(time.time() - t0)\n",
    "    return np.mean(train_mae), np.mean(train_rmse), np.mean(val_mae), np.mean(\n",
    "        val_rmse), test_mae, test_rmse, test_mape, np.mean(train_acc)\n",
    "\n",
    "\n",
    "def test():\n",
    "    if type == 'pretrain':\n",
    "        domain_classifier.eval()\n",
    "    model.eval()\n",
    "\n",
    "    test_mape, test_rmse, test_mae = list(), list(), list()\n",
    "\n",
    "    for i, (feat, label) in enumerate(test_dataloader.get_iterator()):\n",
    "        feat = torch.FloatTensor(feat).to(device)\n",
    "        label = torch.FloatTensor(label).to(device)\n",
    "        mask = select_mask(feat.shape[2])\n",
    "        if torch.sum(scaler.inverse_transform(label)) <= 0.001:\n",
    "            continue\n",
    "\n",
    "        pred = model(vec_pems04, vec_pems07, vec_pems08, feat, True)\n",
    "        pred = pred.transpose(1, 2).reshape((-1, feat.size(2)))\n",
    "        label = label.reshape((-1, label.size(2)))\n",
    "\n",
    "        mae_test, rmse_test, mape_test = masked_loss(scaler.inverse_transform(pred), scaler.inverse_transform(label),\n",
    "                                                     maskp=mask)\n",
    "\n",
    "        test_mae.append(mae_test.item())\n",
    "        test_rmse.append(rmse_test.item())\n",
    "        test_mape.append(mape_test.item())\n",
    "\n",
    "    test_rmse = np.mean(test_rmse)\n",
    "    test_mae = np.mean(test_mae)\n",
    "    test_mape = np.mean(test_mape)\n",
    "\n",
    "    return test_mae, test_rmse, test_mape\n",
    "\n",
    "\n",
    "def model_train(args, model, optimizer):\n",
    "    dur = []\n",
    "    epoch = 1\n",
    "    best = 999999999999999\n",
    "    acc = list()\n",
    "\n",
    "    step_per_epoch = train_dataloader.get_num_batch()\n",
    "    total_step = 200 * step_per_epoch\n",
    "\n",
    "    while epoch <= args.epoch:\n",
    "        start_step = epoch * step_per_epoch\n",
    "        if type == 'fine-tune' and epoch > 1000:\n",
    "            args.val = True\n",
    "        mae_train, rmse_train, mae_val, rmse_val, mae_test, rmse_test, mape_test, train_acc = train(dur, model,\n",
    "                                                                                                    optimizer,\n",
    "                                                                                                    total_step,\n",
    "                                                                                                    start_step)\n",
    "        log(f'Epoch {epoch} | acc_train: {train_acc: .4f} | mae_train: {mae_train: .4f} | rmse_train: {rmse_train: .4f} | mae_val: {mae_val: .4f} | rmse_val: {rmse_val: .4f} | mae_test: {mae_test: .4f} | rmse_test: {rmse_test: .4f} | mape_test: {mape_test: .4f} | Time(s) {dur[-1]: .4f}')\n",
    "        epoch += 1\n",
    "        acc.append(train_acc)\n",
    "        if mae_val <= best:\n",
    "            if type == 'fine-tune' and mae_val > 0.001:\n",
    "                best = mae_val\n",
    "                state = dict([('model', copy.deepcopy(model.state_dict())),\n",
    "                              ('optim', copy.deepcopy(optimizer.state_dict())),\n",
    "                              ('domain_classifier', copy.deepcopy(domain_classifier.state_dict()))])\n",
    "                cnt = 0\n",
    "            elif type == 'pretrain':\n",
    "                best = mae_val\n",
    "                state = dict([('model', copy.deepcopy(model.state_dict())),\n",
    "                              ('optim', copy.deepcopy(optimizer.state_dict())),\n",
    "                              ('domain_classifier', copy.deepcopy(domain_classifier.state_dict()))])\n",
    "                cnt = 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "        if cnt == args.patience or epoch > args.epoch:\n",
    "            print(f'Stop!!')\n",
    "            print(f'Avg acc: {np.mean(acc)}')\n",
    "            break\n",
    "    print(\"Optimization Finished!\")\n",
    "    return state\n",
    "\n",
    "\n",
    "args = arg_parse(argparse.ArgumentParser())\n",
    "print(args)\n",
    "device = torch.device(\"cuda:\" + str(args.device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')\n",
    "if args.c != \"default\":\n",
    "    c = ast.literal_eval(args.c)\n",
    "    record = ResearchRecord(**c)\n",
    "    record_id = record.insert(__file__, get_timestamp(), args.__str__())\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if args.labelrate > 100:\n",
    "    args.labelrate = 100\n",
    "\n",
    "from funcs import add_self_loop, idx_1d22d\n",
    "def load_all_adj(device):\n",
    "    dirs = \"./data/{}/{}_roads.npy\"\n",
    "    ny, chi, dc = None, None, None\n",
    "    for i in [\"BJ\", \"BJ\", \"DC\"]:\n",
    "        t = dirs.format(i, i)\n",
    "        t = np.load(t)\n",
    "        t = t.reshape((t.shape[0] * t.shape[1], t.shape[0] * t.shape[1]))\n",
    "        t = np.where(t >= 1, 1, t)\n",
    "        t = add_self_loop(t)\n",
    "        for m in range(t.shape[0]):\n",
    "            for n in range(t.shape[1]):\n",
    "                a, b = idx_1d22d(m, t.shape)\n",
    "                c, d = idx_1d22d(n, t.shape)\n",
    "                dis = abs(a - c) + abs(b - d)\n",
    "                if t[m][n] - 0 > 1e-6 and dis != 0:\n",
    "                    t[m][n] = t[m][n] / dis\n",
    "        if t.shape[0] == 1024:\n",
    "            ny = t\n",
    "        elif t.shape[0] == 476:\n",
    "            chi = t\n",
    "        elif t.shape[0] == 420:\n",
    "            dc = t\n",
    "\n",
    "    return torch.tensor(ny).to(device), torch.tensor(chi).to(device), torch.tensor(dc).to(device)\n",
    "\n",
    "\n",
    "\n",
    "adj_pems04, adj_pems07, adj_pems08 = load_all_adj(device)\n",
    "vec_pems04 = vec_pems07 = vec_pems08 = None, None, None\n",
    "dc = np.load(\"./data/DC/{}DC_{}.npy\".format(args.dataname, args.datatype))\n",
    "dcmask = dc.sum(0) > 0\n",
    "\n",
    "chi = np.load(\"./data/CHI/{}CHI_{}.npy\".format(args.dataname, args.datatype))\n",
    "chimask = chi.sum(0) > 0\n",
    "\n",
    "ny = np.load(\"./data/NY/{}NY_{}.npy\".format(args.dataname, args.datatype))\n",
    "nymask = ny.sum(0) > 0\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "if cur_dir[-2:] == 'sh':\n",
    "    cur_dir = cur_dir[:-2]\n",
    "\n",
    "pems04_emb_path = os.path.join('{}'.format(cur_dir), 'embeddings', 'node2vec', 'pems04',\n",
    "                               '{}_vecdim.pkl'.format(args.vec_dim))\n",
    "pems07_emb_path = os.path.join('{}'.format(cur_dir), 'embeddings', 'node2vec', 'pems07',\n",
    "                               '{}_vecdim.pkl'.format(args.vec_dim))\n",
    "pems08_emb_path = os.path.join('{}'.format(cur_dir), 'embeddings', 'node2vec', 'pems08',\n",
    "                               '{}_vecdim.pkl'.format(args.vec_dim))\n",
    "\n",
    "for i in [pems04_emb_path, pems07_emb_path, pems08_emb_path]:\n",
    "    a = i.split(\"/\")\n",
    "    b = []\n",
    "    for i in a:\n",
    "        if \"pkl\" in i:\n",
    "            continue\n",
    "        else:\n",
    "            b.append(i)\n",
    "    local_path_generate(folder_name=\"/\".join(b), create_folder_only=True)\n",
    "\n",
    "if os.path.exists(pems04_emb_path):\n",
    "    print(f'Loading pems04 embedding...')\n",
    "    vec_pems04 = torch.load(pems04_emb_path, map_location='cpu')\n",
    "    vec_pems04 = vec_pems04.to(device)\n",
    "else:\n",
    "    print(f'Generating pems04 embedding...')\n",
    "    args.dataset = '4'\n",
    "    vec_pems04, _ = generate_vector(adj_pems04.cpu().numpy(), args)\n",
    "    vec_pems04 = vec_pems04.to(device)\n",
    "    print(f'Saving pems04 embedding...')\n",
    "    torch.save(vec_pems04.cpu(), pems04_emb_path)\n",
    "\n",
    "if os.path.exists(pems07_emb_path):\n",
    "    print(f'Loading pems07 embedding...')\n",
    "    vec_pems07 = torch.load(pems07_emb_path, map_location='cpu')\n",
    "    vec_pems07 = vec_pems07.to(device)\n",
    "else:\n",
    "    print(f'Generating pems07 embedding...')\n",
    "    args.dataset = '7'\n",
    "    vec_pems07, _ = generate_vector(adj_pems07.cpu().numpy(), args)\n",
    "    vec_pems07 = vec_pems07.to(device)\n",
    "    print(f'Saving pems07 embedding...')\n",
    "    torch.save(vec_pems07.cpu(), pems07_emb_path)\n",
    "\n",
    "if os.path.exists(pems08_emb_path):\n",
    "    print(f'Loading pems08 embedding...')\n",
    "    vec_pems08 = torch.load(pems08_emb_path, map_location='cpu')\n",
    "    vec_pems08 = vec_pems08.to(device)\n",
    "else:\n",
    "    print(f'Generating pems08 embedding...')\n",
    "    args.dataset = '8'\n",
    "    vec_pems08, _ = generate_vector(adj_pems08.cpu().numpy(), args)\n",
    "    vec_pems08 = vec_pems08.to(device)\n",
    "    print(f'Saving pems08 embedding...')\n",
    "    torch.save(vec_pems08.cpu(), pems08_emb_path)\n",
    "\n",
    "print(f'Successfully load embeddings, 4: {vec_pems04.shape}, 7: {vec_pems07.shape}, 8: {vec_pems08.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "domain_criterion = torch.nn.NLLLoss()\n",
    "domain_classifier = Domain_classifier_DG(num_class=3, encode_dim=args.enc_dim)\n",
    "\n",
    "domain_classifier = domain_classifier.to(device)\n",
    "state = g = None, None\n",
    "\n",
    "batch_seen = 0\n",
    "cur_dir = os.getcwd()\n",
    "if cur_dir[-2:] == 'sh':\n",
    "    cur_dir = cur_dir[:-2]\n",
    "assert args.model in [\"DASTNet\"]\n",
    "\n",
    "bak_epoch = args.epoch\n",
    "bak_val = args.val\n",
    "bak_test = args.test\n",
    "type = 'pretrain'\n",
    "pretrain_model_path = os.path.join('{}'.format(cur_dir), 'pretrained', 'transfer_models',\n",
    "                                   '{}'.format(args.dataset), '{}_prelen'.format(args.pre_len),\n",
    "                                   'flow_model4_{}_epoch_{}{}{}.pkl'.format(args.model, args.epoch,\n",
    "                                                                            args.dataname,\n",
    "                                                                            args.datatype))\n",
    "\n",
    "a = pretrain_model_path.split(\"/\")\n",
    "b = []\n",
    "for i in a:\n",
    "    if \"pkl\" not in i:\n",
    "        b.append(i)\n",
    "local_path_generate(\"/\".join(b), create_folder_only=True)\n",
    "\n",
    "if os.path.exists(pretrain_model_path):\n",
    "    print(f'Loading pretrained model at {pretrain_model_path}')\n",
    "    state = torch.load(pretrain_model_path, map_location='cpu')\n",
    "else:\n",
    "    print(f'No existing pretrained model at {pretrain_model_path}')\n",
    "    args.val = args.test = False\n",
    "    datasets = [\"4\"]\n",
    "    dataset_bak = args.dataset\n",
    "    labelrate_bak = args.labelrate\n",
    "    args.labelrate = 100\n",
    "    dataset_count = 0\n",
    "\n",
    "    for dataset in [item for item in datasets if item not in [dataset_bak]]:\n",
    "        dataset_count = dataset_count + 1\n",
    "\n",
    "        print(\n",
    "            f'\\n\\n****************************************************************************************************************')\n",
    "        print(f'dataset: {dataset}, model: {args.model}, pre_len: {args.pre_len}, labelrate: {args.labelrate}')\n",
    "        print(\n",
    "            f'****************************************************************************************************************\\n\\n')\n",
    "\n",
    "        if dataset == '4':\n",
    "            g = vec_pems04\n",
    "        elif dataset == '7':\n",
    "            g = vec_pems07\n",
    "        elif dataset == '8':\n",
    "            g = vec_pems08\n",
    "\n",
    "        args.dataset = dataset\n",
    "        train_dataloader, val_dataloader, test_dataloader, adj, max_speed, scaler = load_data(args)\n",
    "        model = DASTNet(input_dim=args.vec_dim, hidden_dim=args.hidden_dim, encode_dim=args.enc_dim,\n",
    "                        device=device, batch_size=args.batch_size, etype=args.etype, pre_len=args.pre_len,\n",
    "                        dataset=args.dataset, ft_dataset=dataset_bak,\n",
    "                        adj_pems04=adj_pems04, adj_pems07=adj_pems07, adj_pems08=adj_pems08).to(device)\n",
    "        optimizer = optim.SGD([{'params': model.parameters()},\n",
    "                               {'params': domain_classifier.parameters()}], lr=args.learning_rate, momentum=0.8)\n",
    "\n",
    "        if dataset_count != 1:\n",
    "            model.load_state_dict(state['model'])\n",
    "            optimizer.load_state_dict(state['optim'])\n",
    "\n",
    "        state = model_train(args, model, optimizer)\n",
    "\n",
    "    print(f'Saving model to {pretrain_model_path} ...')\n",
    "    torch.save(state, pretrain_model_path)\n",
    "    args.dataset = dataset_bak\n",
    "    args.labelrate = labelrate_bak\n",
    "    args.val = bak_val\n",
    "    args.test = bak_test\n",
    "\n",
    "type = 'fine-tune'\n",
    "args.epoch = args.fine_epoch\n",
    "\n",
    "print(f'\\n\\n*******************************************************************************************')\n",
    "print(\n",
    "    f'dataset: {args.dataset}, model: {args.model}, pre_len: {args.pre_len}, labelrate: {args.labelrate}, seed: {args.division_seed}')\n",
    "print(f'*******************************************************************************************\\n\\n')\n",
    "\n",
    "if args.dataset == '4':\n",
    "    g = vec_pems04\n",
    "elif args.dataset == '7':\n",
    "    g = vec_pems07\n",
    "elif args.dataset == '8':\n",
    "    g = vec_pems08\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader, adj, max_speed, scaler = load_data(args, cut=True)\n",
    "model = DASTNet(input_dim=args.vec_dim, hidden_dim=args.hidden_dim, encode_dim=args.enc_dim,\n",
    "                device=device, batch_size=args.batch_size, etype=args.etype, pre_len=args.pre_len,\n",
    "                dataset=args.dataset, ft_dataset=args.dataset,\n",
    "                adj_pems04=adj_pems04, adj_pems07=adj_pems07, adj_pems08=adj_pems08).to(device)\n",
    "optimizer = optim.SGD([{'params': model.parameters()},\n",
    "                       {'params': domain_classifier.parameters()}], lr=args.learning_rate, momentum=0.8)\n",
    "model.load_state_dict(state['model'])\n",
    "optimizer.load_state_dict(state['optim'])\n",
    "\n",
    "if args.labelrate != 0:\n",
    "    test_state = model_train(args, model, optimizer)\n",
    "    model.load_state_dict(test_state['model'])\n",
    "    optimizer.load_state_dict(test_state['optim'])\n",
    "\n",
    "test_mae, test_rmse, test_mape = test()\n",
    "print(f'mae: {test_mae: .4f}, rmse: {test_rmse: .4f}, mape: {test_mape * 100: .4f}\\n\\n')\n",
    "if args.c != \"default\":\n",
    "    if args.need_remark == 1:\n",
    "        record.update(record_id, get_timestamp(),\n",
    "                      \"%.4f,%.4f,%.4f\" %\n",
    "                      (test_rmse, test_mae, test_mape * 100),\n",
    "                      remark=\"{}C {} {} {} {}\".format(\"2\" if args.need_third == 0 else \"3\", str(args.data_amount),\n",
    "                                                      args.dataname, args.datatype, args.machine_code))\n",
    "    else:\n",
    "        record.update(record_id, get_timestamp(),\n",
    "                      \"%.4f,%.4f, %.4f\" %\n",
    "                      (test_rmse, test_mae, test_mape * 100),\n",
    "                      remark=\"{}\".format(args.machine_code))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
